{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy/HDBScan Feature Extraction Pipeline\n",
    "\n",
    "### Note: it can be quite complicated to install spaCy and sense2vec, given conflicting low-level requirements, so at this point I wouldn't suggest that others try to install the libraries and run this notebook.\n",
    "  \n",
    "However, it is well worth scanning down to the cell titled ***Harvesting Word Features***. In the output of that cell, there are examples of 52 feature clusters harvested by this process. The ultimate output of this process will produce a dataset containing a product ID (asin), overall rating, and word feature, for each word feature found in each product review. I don't consider these feature clusters as the final product, and we should discuss.\n",
    "\n",
    "\n",
    "### We can use this output for several purposes. \n",
    "\n",
    "1. First, we should be able to quite easily make the data available to th web interface, so that we can display the top n word features (by overall rating) associated with products returned.\n",
    "\n",
    "2. We will want to also include the user's selected word features in our model evaluation, to enable them to \"drill into\" selected features and thus explore the product/feature landscape.\n",
    "\n",
    "3. Finally, I think it would be worth training a model on a vectorized representation of the top n most highly rated features, which may give us another dimension for predicting rating based on feature combination/interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import time\n",
    "# Install a few python packages using pip\n",
    "from common import utils\n",
    "utils.require_package(\"wget\")      # for fetching dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard python helper libraries.\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os, sys, time\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "# Numerical manipulation libraries.\n",
    "import numpy as np\n",
    "\n",
    "#Visualization\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import spacy\n",
    "#activated = spacy.prefer_gpu()\n",
    "\n",
    "import hdbscan\n",
    "import seaborn as sns\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "from spacy.tokens import Doc\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading English core web medium language data using spaCy...\n",
      "...finished reading English language model 'en_core_web_md' in 12.893219947814941 seconds.\n"
     ]
    }
   ],
   "source": [
    "plotting = True\n",
    "labels_words = False\n",
    "\n",
    "language_model = 'en_core_web_md'\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "print(\"Reading English core web medium language data using spaCy...\")\n",
    "nlp = spacy.load(language_model)\n",
    "print(\"...finished reading English language model '{}' in {} seconds.\".format(language_model, time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "lemmas = {}\n",
    "ignore_words = []\n",
    "\n",
    "\n",
    "path_for_tf_metadata = './logdir/embedding_test'\n",
    "path_for_tf_ckpt = path_for_tf_metadata+'/embedding_test.ckpt'\n",
    "vectors_filepath = './data/vectors_each.tsv'\n",
    "metadata_filepath = './data/metadata_each.tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Pandas dataframe from reviews_Toys_and_Games_5.json.gz...\n",
      "start getDF\n",
      "start parse\n",
      "end parse with time for parse 7.405748128890991\n",
      "end getDF\n",
      "time taken to load data =  7.406217098236084\n",
      "...read reviews_Toys_and_Games_5.json.gz in 9.916450023651123 seconds.\n"
     ]
    }
   ],
   "source": [
    "def parse(path):\n",
    "  print('start parse')\n",
    "  start_parse = time.time()\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "  end_parse = time.time()\n",
    "  print('end parse with time for parse',end_parse - start_parse)\n",
    "\n",
    "def getDF(path):\n",
    "  print('start getDF')\n",
    "  start = time.time()\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  print('end getDF')\n",
    "  end = time.time()\n",
    "  print('time taken to load data = ',end-start)\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "start = time.time()\n",
    "print(\"Reading Pandas dataframe from reviews_Toys_and_Games_5.json.gz...\")\n",
    "df = getDF('./data/reviews_Toys_and_Games_5.json.gz')\n",
    "print(\"...read reviews_Toys_and_Games_5.json.gz in {} seconds.\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167597, 9)\n",
      "Index(['summary', 'unixReviewTime', 'reviewerName', 'helpful', 'reviewText',\n",
      "       'asin', 'reviewTime', 'overall', 'reviewerID'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magnetic board</td>\n",
       "      <td>1390953600</td>\n",
       "      <td>Angie</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I like the item pricing. My granddaughter want...</td>\n",
       "      <td>0439893577</td>\n",
       "      <td>01 29, 2014</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A1VXOAVRGKGEAK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it works pretty good for moving to different a...</td>\n",
       "      <td>1395964800</td>\n",
       "      <td>Candace</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Love the magnet easel... great for moving to d...</td>\n",
       "      <td>0439893577</td>\n",
       "      <td>03 28, 2014</td>\n",
       "      <td>4.0</td>\n",
       "      <td>A8R62G708TSCM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary  unixReviewTime  \\\n",
       "0                                     Magnetic board      1390953600   \n",
       "1  it works pretty good for moving to different a...      1395964800   \n",
       "\n",
       "  reviewerName helpful                                         reviewText  \\\n",
       "0        Angie  [0, 0]  I like the item pricing. My granddaughter want...   \n",
       "1      Candace  [1, 1]  Love the magnet easel... great for moving to d...   \n",
       "\n",
       "         asin   reviewTime  overall      reviewerID  \n",
       "0  0439893577  01 29, 2014      5.0  A1VXOAVRGKGEAK  \n",
       "1  0439893577  03 28, 2014      4.0   A8R62G708TSCM  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.columns)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a4ac7ed68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Number of reviews by product\n",
    "cnt_by_product = df[['asin','reviewText']].groupby('asin').agg('count')\n",
    "cnt_by_product['reviewText'].hist(bins = [0,1,2,3,4,5,10,15,20,50,100,500,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_suspect_tokens(text, nlp, ignore_words, lemmas, ignore_initial=False):\n",
    "    \"\"\" Parse text and return ignored words at the begining of the phrase, along with the valid phrase (if any)\n",
    "    at the end of the phrase token.\n",
    "    \n",
    "    Args\n",
    "    ----------\n",
    "    ignore_initial (boolean) indicator of whether to ignore the first word in the phrase token (when a recursive call)\n",
    "    text (string)            text to be parsed, tokenized, and vectorized\n",
    "    nlp (spaCy pipeline)     pipeline to use for processing the input text\n",
    "    ignore_words (list)      collected set of words to be ignored, to which this function may add words\n",
    "    lemmas (dict)            dict of word->\n",
    "    \n",
    "    Returns:\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if ignore_initial:\n",
    "        token_parts = text.split(\" \")\n",
    "        first_word = token_parts[0]\n",
    "        \n",
    "        if first_word not in ignore_words:\n",
    "            ignore_words.append(first_word)\n",
    "            #print(\"Ignoring word '{}' in feature extraction...\".format(first_word))    \n",
    "        \n",
    "        if (\" \" in text) and (len(text.split(\" \"))>1):\n",
    "            text = \" \".join(token_parts[1:])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        if debug:\n",
    "            print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop, [child for child in token.children])\n",
    "        lower_token = token.text.lower()\n",
    "        if (lower_token is not None):\n",
    "            if (lower_token not in ignore_words):\n",
    "                if (token.lemma_ in IGNORED_LEMMAS) or (token.pos_ in IGNORED_POS):\n",
    "                    consume_suspect_tokens(text, nlp, ignore_words, lemmas, ignore_initial=True)\n",
    "                else:\n",
    "                    lemmas[token.text] = lower_token\n",
    "                \n",
    "        return None\n",
    "\n",
    "\n",
    "def get_vectors(text, nlp, ignore_words, lemmas):\n",
    "    \"\"\" <generator> Get embedding word vectors from a given text object. \n",
    "    Args\n",
    "    ----------\n",
    "    text (string)            text to be parsed, tokenized, and vectorized\n",
    "    nlp (spaCy pipeline)     pipeline to use for processing the input text\n",
    "    ignore_words (list)      collected set of words to be ignored, to which this function may add words\n",
    "    lemmas (dict)            dict of word->\n",
    "    \n",
    "    Generates:\n",
    "    ----------\n",
    "    processed text (string) \n",
    "    phrase vector (numpy.ndarray)\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks_seen = []\n",
    "    consume_suspect_tokens(text, nlp, ignore_words, lemmas)\n",
    "              \n",
    "    doc = nlp(text)\n",
    "    #####\n",
    "    # Next, iterate through the sentences and within those the noun chunks.\n",
    "    # These noun chunks will be lemmatized and collected as potential features.\n",
    "    #####\n",
    "    for sent in doc.sents:\n",
    "        for chunk in sent.noun_chunks:\n",
    "           \n",
    "            if chunk.text not in chunks_seen:\n",
    "                chunks_seen.append(chunk.text)\n",
    "                processed_text = chunk.text\n",
    "                \n",
    "                lemmatized_tokens = []\n",
    "                \n",
    "                if lemmas is not None:\n",
    "                    \n",
    "                    for chunk_token in chunk.text.split(' '):\n",
    "                        lower_token = chunk_token.lower()\n",
    "                        if (ignore_words is None) or (lower_token not in ignore_words):\n",
    "                            \n",
    "                            try:\n",
    "                                this_lemma = lemmas[lower_token]\n",
    "                            except:\n",
    "                                this_lemma = lower_token\n",
    "                                \n",
    "                            lemmatized_tokens.append(this_lemma)\n",
    "                            \n",
    "                    if len(lemmatized_tokens)>0:\n",
    "                        processed_text = \" \".join(lemma for lemma in lemmatized_tokens)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                yield processed_text, chunk.vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vectors(product, rating, concept_vec):\n",
    "    \"\"\"Write product, rating, phrase and sense vector to file\"\"\"\n",
    "    \n",
    "    with open(vectors_filepath, 'a') as out_v, open(metadata_filepath, 'a') as out_m:\n",
    "        phrase = concept_vec[0]\n",
    "        sense_vector = concept_vec[1]\n",
    "        out_m.write('{}\\t{}\\t{}\\n'.format(product, str(rating),phrase))\n",
    "        out_v.write('\\t'.join([str(x) for x in sense_vector]) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              reviewerID  reviewText\n",
      "0  A012468118FTQAINEI0OQ           6\n",
      "1   A0182108CPDLPRCXQUZQ           8\n",
      "2  A026961431MGW0616BRS3           6\n",
      "3  A034597326Z83X79S50FI           7\n",
      "4  A04295422T2ZG087R17FX           5\n",
      "         asin  reviewText\n",
      "0  0439893577          17\n",
      "1  048645195X          11\n",
      "2  0545496470           6\n",
      "3  0615444172           6\n",
      "4  0670010936          10\n"
     ]
    }
   ],
   "source": [
    "#Number of reviews by reviewer\n",
    "cnt_by_reviewer = df[['reviewerID','reviewText']].groupby('reviewerID').agg('count').reset_index()\n",
    "\n",
    "good_reviewers = cnt_by_reviewer[cnt_by_reviewer['reviewText']>4]\n",
    "print(good_reviewers[:5])\n",
    "\n",
    "good_products = cnt_by_product[cnt_by_product['reviewText']>4].reset_index()\n",
    "print(good_products[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample vect[(300,)]\n",
      "There are 167597 total reviews for reviewers with at least 5 reviews each and products with at least 5 reviews each\n",
      "\n",
      "Collecting word concept vectors for 1001 reviews...\n",
      "Starting iteration over reviews 0-200...\n",
      "...completed processing 200 reviews in 12.119228839874268 seconds.\n",
      "Starting iteration over reviews 200-400...\n",
      "...completed processing 200 reviews in 14.87395691871643 seconds.\n",
      "Starting iteration over reviews 400-600...\n",
      "...completed processing 200 reviews in 12.465951919555664 seconds.\n",
      "Starting iteration over reviews 600-800...\n",
      "...completed processing 200 reviews in 15.583565950393677 seconds.\n",
      "Starting iteration over reviews 800-1000...\n",
      "...completed processing 200 reviews in 12.564543008804321 seconds.\n",
      "...Collected 800 word vectors in 67.73524498939514 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Here are the controls for ignored words in the review\n",
    "IGNORED_LEMMAS = ['-PRON-']\n",
    "IGNORED_POS = ['PUNCT', 'SPACE', 'PART', 'DET']\n",
    "\n",
    "write_to_file = True\n",
    "\n",
    "!rm -f ./vectors_each.tsv\n",
    "!rm -f ./metadata_each.tsv\n",
    "\n",
    "sample_vect = [vec for vec in get_vectors(\"example\", nlp, ignore_words, lemmas)][0][1]\n",
    "vect_dim = sample_vect.shape\n",
    "print(\"Sample vect[{}]\".format(vect_dim))\n",
    "index = []\n",
    "output = None\n",
    "\n",
    "total_start = time.time()\n",
    "\n",
    "good_reviews = df[df['reviewerID'].isin(good_reviewers['reviewerID'])][df['asin'].isin(good_products['asin'])]\n",
    "\n",
    "print(\"There are {} total reviews for reviewers with at least 5 reviews each and products with at least 5 reviews each\".format(len(good_reviews)))\n",
    "\n",
    "iteration_size = 200\n",
    "#iter_limit = len(good_reviews)\n",
    "iter_limit = 1001\n",
    "\n",
    "\n",
    "print(\"\\nCollecting word concept vectors for {} reviews...\".format(iter_limit))\n",
    "\n",
    "for iteration in range(int(iter_limit/iteration_size)):\n",
    "\n",
    "    print(\"Starting iteration over reviews {}-{}...\".format(iteration*iteration_size,\n",
    "                                                            (iteration+1)*iteration_size))\n",
    "    iter_start_time = time.time()\n",
    "\n",
    "    for iter_ind in range(iteration_size):\n",
    "    \n",
    "        review_ind = iteration*iteration_size + iter_ind\n",
    "        \n",
    "        reviewer = good_reviews['reviewerID'].iloc[review_ind]\n",
    "        product = good_reviews['asin'].iloc[review_ind]\n",
    "    \n",
    "        rating = good_reviews['overall'].iloc[review_ind]\n",
    "        review = good_reviews['reviewText'].iloc[review_ind]\n",
    "    \n",
    "        #print(review)\n",
    "        for concept_vec in get_vectors(review, nlp, ignore_words, lemmas):\n",
    "            \n",
    "            if write_to_file:\n",
    "                # Append data to files for later reading\n",
    "                write_vectors(product, rating, concept_vec)\n",
    "            else:\n",
    "                # Append data to a list and a numpy array\n",
    "                index.append([product, rating, concept_vec[0]])\n",
    "        \n",
    "                if output is None:\n",
    "                    # Create an np.array with the first row as the retrieved word vector\n",
    "                    output = np.array([concept_vec[1]])\n",
    "                else:\n",
    "                    # Append the next vector to the end of the vectors array\n",
    "                    output = np.append(output, np.array([concept_vec[1]]), axis=0)            \n",
    "\n",
    "    print(\"...completed processing {} reviews in {} seconds.\".format(iteration_size, time.time()-iter_start_time))\n",
    "    \n",
    "print(\"...Collected {} word vectors in {} seconds.\".format(iteration*iteration_size, time.time()-total_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors file contains 957864 lines\n",
      "This sample probability will include 9621 samples from the generated sense vectors.\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "vdim = file_len(vectors_filepath)\n",
    "print(\"vectors file contains {} lines\".format(vdim))\n",
    "index = []\n",
    "output = None\n",
    "\n",
    "sample_prob=0.01\n",
    "# Generate random samples set, used for both the vectors file and the metadata file so the elements match\n",
    "samples = np.random.choice(a=[True,False], size=vdim, p=[sample_prob,1.0-sample_prob])\n",
    "print(\"This sample probability will include {} samples from the generated sense vectors.\".format(sum(samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9413/957864 [00:00<00:10, 94129.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ./data/vectors_each.tsv contains sense vectors of dimension 957864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 957864/957864 [06:35<00:00, 2419.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (9621, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the vectors file back in, in case you want to restart from here.\n",
    "\n",
    "with open(vectors_filepath, 'r') as in_v:\n",
    "    print('File {} contains {} sense vectors.'.format(vectors_filepath, vdim))\n",
    "    curr_line = 0\n",
    "    t = tqdm.tqdm(total=vdim)\n",
    "    \n",
    "    for line in in_v:\n",
    "        sample_this_row = samples[curr_line]\n",
    "        \n",
    "        if sample_this_row:\n",
    "            if output is None:\n",
    "                # Create an np.array with the first row as the retrieved word vector\n",
    "                output = np.array([np.array(line.split('\\t'))])\n",
    "            else:\n",
    "                # Append the next vector to the end of the vectors array\n",
    "                output = np.append(output, np.array([np.array(line.split('\\t'))]), axis=0)\n",
    "                \n",
    "        curr_line += 1\n",
    "        t.update()\n",
    "        #if len(output)>2:        \n",
    "        #    break\n",
    "    t.close()\n",
    "\n",
    "\n",
    "print(\"Output shape: {}\".format(output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating word clusters from word vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/burgew/miniconda3/envs/spacy3.5/lib/python3.5/site-packages/sklearn/utils/validation.py:558: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
      "  FutureWarning)\n",
      "/Users/burgew/miniconda3/envs/spacy3.5/lib/python3.5/site-packages/sklearn/utils/validation.py:558: FutureWarning: Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...completed clustering in 57.96712398529053 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Fit an HDBScan model using the sampled sense vectors\n",
    "\n",
    "HDBSCAN_METRIC = 'manhattan'\n",
    "\n",
    "start = time.time()\n",
    "print(\"Creating word clusters from word vectors...\")\n",
    "hdbscanner = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, \n",
    "                             metric=HDBSCAN_METRIC, gen_min_span_tree=True, prediction_data=True)\n",
    "hdbscanner.fit(output)\n",
    "print(\"...completed clustering in {} seconds.\".format(time.time()-start))\n",
    "\n",
    "\n",
    "import pickle \n",
    "with open('./data/hdbscanner.{}.pickle'.format(len(output)), 'wb') as pickle_file:\n",
    "    pickle.dump(hdbscanner, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condensing the linkage tree and then plotting...\n",
      "...plotted condensed tree in 2.2661709785461426 seconds.\n",
      "Found 115 clusters\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the condensed cluster tree\n",
    "\n",
    "if plotting:\n",
    "\tstart = time.time()\n",
    "\tprint(\"Condensing the linkage tree and then plotting...\")\n",
    "\t#hdbscanner.single_linkage_tree_.plot(cmap='viridis', colorbar=True)\n",
    "\thdbscanner.condensed_tree_.plot()\n",
    "\thdbscanner.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette())\n",
    "\tprint(\"...plotted condensed tree in {} seconds.\".format(time.time()-start))\n",
    "\ttree = hdbscanner.condensed_tree_\n",
    "\tprint(\"Found {} clusters\".format(len(tree._select_clusters())))\n",
    "\tmatplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exemplars(cluster_id, condensed_tree):\n",
    "    \"\"\" Collect and return the exemplar words for each cluster. \"\"\"\n",
    "    raw_tree = condensed_tree._raw_tree\n",
    "    # Just the cluster elements of the tree, excluding singleton points\n",
    "    cluster_tree = raw_tree[raw_tree['child_size'] > 1]\n",
    "    # Get the leaf cluster nodes under the cluster we are considering\n",
    "    leaves = hdbscan.plots._recurse_leaf_dfs(cluster_tree, cluster_id)\n",
    "    # Now collect up the last remaining points of each leaf cluster (the heart of the leaf)\n",
    "    result = np.array([])\n",
    "    for leaf in leaves:\n",
    "        max_lambda = raw_tree['lambda_val'][raw_tree['parent'] == leaf].max()\n",
    "        points = raw_tree['child'][(raw_tree['parent'] == leaf) & \n",
    "                                   (raw_tree['lambda_val'] == max_lambda)]\n",
    "        result = np.hstack((result, points))\n",
    "    return result.astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting clusters in tree...\n",
      "...finished selecting clusters in 0.020389080047607422 seconds.\n",
      "Found 115 clusters\n",
      "File ./data/metadata_each.tsv contains index entries of of dimension 957864\n",
      "Index(28863, 3): [['0439893577', '4.0', 'it'], ['048645195X', '5.0', 'these designs'], ['0615444172', '5.0', 'best incentive'], ['0735321396', '4.0', 'grandson'], ['0735321396', '5.0', 'it']]\n",
      "All Points(957864, 3): [['0439893577', '5.0', 'i'], ['0439893577', '5.0', 'item pricing'], ['0439893577', '5.0', 'granddaughter'], ['0439893577', '5.0', 'it'], ['0439893577', '5.0', 'letters']]\n",
      "File ./data/metadata_each.tsv contains 957864 index entries.\n"
     ]
    }
   ],
   "source": [
    "tree = hdbscanner.condensed_tree_\n",
    "\n",
    "#print('Index, for reference:')\n",
    "#for ind, entry in enumerate(index):\n",
    "#    print(\"cluster: {}, ind: {}, entry: {}\".format(hdbscanner.labels_[ind], ind, entry))\n",
    "\n",
    "start = time.time()\n",
    "print(\"Selecting clusters in tree...\")\n",
    "clusters = tree._select_clusters()\n",
    "print(\"...finished selecting clusters in {} seconds.\".format(time.time()-start))\n",
    "\n",
    "initial_cluster_count = len(clusters)\n",
    "print(\"Found {} clusters\".format(initial_cluster_count))\n",
    "\n",
    "all_points = []\n",
    "labels = []\n",
    "\n",
    "# iterate through the input metadata once, to collect all words and the labels for the sampled points\n",
    "with open(metadata_filepath, 'r') as in_m, open(path_for_tf_metadata+'/metadata.tsv', 'w') as out_tf_meta:\n",
    "\tmdim = None\n",
    "\tcurr_line = 0\n",
    "\tfor line in in_m:\n",
    "\t\tif mdim is None:\n",
    "\t\t\tmdim = line.count('\\t')+1\n",
    "\t\t\tprint('File {} contains index entries of of dimension {}'.format(metadata_filepath, vdim))\n",
    "\t\tif line.endswith('\\n'):\n",
    "\t\t\tline = line[:-1]\n",
    "\t\tall_points.append(line.split('\\t'))\n",
    "\t\tsample_this_row = samples[curr_line]\n",
    "\t\tif sample_this_row:\n",
    "\t\t\tmeta_line = line.split('\\t')\n",
    "\t\t\tindex.append(meta_line)\n",
    "\t\t\tif labels_words:\n",
    "\t\t\t\tlabels.append(meta_line[2])\n",
    "\t\tcurr_line += 1\n",
    "\n",
    "print(\"Index({}, {}): {}\".format(len(index), len(index[0]), index[:5]))\n",
    "\n",
    "print(\"All Points({}, {}): {}\".format(len(all_points), len(all_points[0]), all_points[:5]))\n",
    "\n",
    "# then, iterate through the input metadata again, to apply the cluster labels, if labels_words is False\n",
    "with open(metadata_filepath, 'r') as in_m, open(path_for_tf_metadata+'/metadata.tsv', 'w') as out_tf_meta:\n",
    "\tmdim = None\n",
    "\tcurr_line = 0\n",
    "\tcurr_sample = 0\n",
    "\n",
    "\tfor line in in_m:\n",
    "\t\tif mdim is None:\n",
    "\t\t\tmdim = line.count('\\t')+1\n",
    "\t\t\tprint('File {} contains {} index entries.'.format(metadata_filepath, vdim))\n",
    "\t\tif line.endswith('\\n'):\n",
    "\t\t\tline = line[:-1]\n",
    "\t\tsample_this_row = samples[curr_line]\n",
    "\t\tif sample_this_row:\n",
    "\t\t\tindex.append(line.split('\\t'))\n",
    "\t\t\tif not labels_words:\n",
    "\t\t\t\tlabels.append(\"-\")\n",
    "\t\t\tcurr_sample += 1\n",
    "\t\tcurr_line += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 20 clusters (17.391304347826086% of initially collected):\n",
      "\n",
      "Exemplars: anyone\n",
      "Members: anyone, anybody, most anyone\n",
      "\n",
      "Exemplars: elf\n",
      "Members: doll, their elf, your elf, elf doll, elf, our elf\n",
      "\n",
      "Exemplars: colors\n",
      "Members: same color, colors, color, six colors, those colors, one color, only one color\n",
      "\n",
      "Exemplars: my daughter, daughter\n",
      "Members: my daughter, youngest daughter, granddaughter, sister, daughters, niece, daughter\n",
      "\n",
      "Exemplars: rulebook, rules\n",
      "Members: basic rules, rules, complicated rules, rule, new rules, rulebook\n",
      "\n",
      "Exemplars: the cards, cards\n",
      "Members: right cards, title card, each card set, more cards, that card, card/deck version, question cards, one card, cards, same cards, two new cards, card loving role, all those cards, most cards, card, larger cards, that one card, several cards, some cards, single card, three cards, question card, these cards, lucky card, card game--, six cards, two cards, destination cards, the cards, four cards, every card, five new cards, each card, ashardalon's card, waterdeep card\n",
      "\n",
      "Exemplars: toy\n",
      "Members: best toys, most other toys, many other toys, other toys, coolest toy, toys, toy, same toy, these toys\n",
      "\n",
      "Exemplars: 2 year old son, 6 year old niece, 7 year old son, 3 year old son, 6 year old son, 3 year old daughter, 9 year old daughter, 10 year old daughter, 7 year old daughter\n",
      "Members: just turned 8 year old daughter, 2 year old son, 7 year old son, her 2.5 yr old sister, 2 1/2 year old son, 3 year old boy, 3 year old daughter, differently able 10 yr old daughter, six year old boys, 5 year old, 9 year old, 30 year old parents, four year old grandson, 6 year old niece, his thirty one year old friend, then 2 1/2 year old son, 4 year old playing, six year old daughter, 7 yr old son, 5 and 6 year old daughters, 6yr old daughter, 19 month old boy, his 20 month old sister, 3 year old and one year old twins, 19 year old son, 3 and 6 year old boys, 6 year old sibling, 10 year old daughter, three year old likes, 4 & 5 last christmas, 14 month son, 7 year old daughter, 3 year old son, 6 year old son, 9 year old daughter, three and five year old girls, our 6 year old struggles\n",
      "\n",
      "Exemplars: kids\n",
      "Members: kids, older kids, younger kids\n",
      "\n",
      "Exemplars: certain number, set number, a number, number\n",
      "Members: set number, set, a number, number, new one, certain number, new place, significant set, new present, different place, new set, different set\n",
      "\n",
      "Exemplars: great gift, gift\n",
      "Members: better gift, great gift, gift, also very fun gift, birthday gift\n",
      "\n",
      "Exemplars: pieces\n",
      "Members: just pieces, pieces, little pieces, all pieces\n",
      "\n",
      "Exemplars: game\n",
      "Members: game play, same boring game, few games, second full game, exact same game, your game, best game, your game play, just few games, first few games, first game, your games, same game, entire game, original game, games, one entire game, game, worst game\n",
      "\n",
      "Exemplars: one\n",
      "Members: one, another time, one time\n",
      "\n",
      "Exemplars: great game\n",
      "Members: great looking game, very nice game, good game, great game, really fun game, still wonderful game, not so much great game, what fun game, great and frustrating game, just fun game, fun family game, such great game, still great game, fun game\n",
      "\n",
      "Exemplars: couple\n",
      "Members: just couple, even couple, couple, quite few non-american ones\n",
      "\n",
      "Exemplars: great time, great one, great way, walk- great, way\n",
      "Members: walk- great, wonderful time, great addition, perfect way, a good time, short time, great one, great way, great time, nice addition, time, long time, way\n",
      "\n",
      "Exemplars: while\n",
      "Members: quite while, good while, while\n",
      "\n",
      "Exemplars: same way, only bad thing, only way, only thing\n",
      "Members: thing, whole thing, all fun, only bad thing, only way, same turn, fun part, other thing, only thing, fun, other fun thing, way, same way\n",
      "\n",
      "Exemplars: time\n",
      "Members: time, first time, very first time, next time, same time\n",
      "\n",
      "There were 5765 words that were considered noise.\n"
     ]
    }
   ],
   "source": [
    "selected_clusters = []\n",
    "\n",
    "for i, c in enumerate(clusters):\n",
    "\tc_exemplars = exemplars(c, tree)\n",
    "\n",
    "\tpoint_label = None\n",
    "\tcluster_exemplars = set()\n",
    "\tfor ind, ex_ind in enumerate(c_exemplars):\n",
    "\t\t#print(\"Exemplar -- {} : {}\".format(index[ex_ind][0], index[ex_ind][2]))\n",
    "\t\tcluster_exemplars.add(index[ex_ind][2])\n",
    "\t\tif point_label is None:\n",
    "\t\t\tpoint_label = index[ex_ind][2]\n",
    "    \n",
    "\tmembers = set()\n",
    "\tfor label_ind, label in np.ndenumerate(hdbscanner.labels_):\n",
    "\t\tif label == i:\n",
    "\t\t\tmembers.add(index[label_ind[0]][2])\n",
    "\t\t\tif not labels_words:\n",
    "\t\t\t\tlabels[label_ind[0]] = point_label\n",
    "            \n",
    "            #print(\"Member: {} : {}\".format(index[label_ind[0]][0], index[label_ind[0]][2]))\n",
    "    \n",
    "\texemplars_len = float(len(cluster_exemplars))\n",
    "\tmembers_len = float(len(members))\n",
    "    \n",
    "    # Look for clusters where the members outnumber the exemplars by 2 times\n",
    "\tif ((exemplars_len>0) and (len(members)>(2.0*exemplars_len))):\n",
    "    \n",
    "\t\texample_cluster_exemplars = \", \".join(cluster_exemplars)\n",
    "\t\texample_cluster_members = \", \".join(members)\n",
    "        \n",
    "\t\tselected_clusters.append([example_cluster_exemplars, example_cluster_members])\n",
    "\n",
    "selected_cluster_count = len(selected_clusters)\n",
    "if (selected_cluster_count>0):\n",
    "    print(\"\\nFound {} clusters ({}% of initially collected):\".\n",
    "          format(len(selected_clusters), 100.0*float(selected_cluster_count)/float(initial_cluster_count)))\n",
    "    for example in selected_clusters:\n",
    "        print(\"\\nExemplars: {}\".format(example[0]))\n",
    "        print(\"Members: {}\".format(example[1]))\n",
    "\n",
    "with open(path_for_tf_metadata+'/metadata.tsv', 'w') as out_tf_meta:\n",
    "\tfor label in labels:\n",
    "\t\tout_tf_meta.write(str(label)+'\\n')\n",
    "                                                                    \n",
    "noise_count = sum([1 for label in hdbscanner.labels_ if label == -1])\n",
    "print(\"\\nThere were {} words that were considered noise.\".format(noise_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting shape:\n",
      "[[ 0.0013629   0.35653    -0.055497   ... -0.11237     0.078259\n",
      "   0.22398   ]\n",
      " [-0.0809065  -0.013005   -0.3085235  ...  0.0526415   0.34908497\n",
      "   0.02017   ]\n",
      " [ 0.13602    -0.031015   -0.0942     ...  0.065075   -0.091585\n",
      "   0.06615   ]\n",
      " ...\n",
      " [-0.1542365   0.04899999 -0.210735   ...  0.2158745   0.2701\n",
      "  -0.217215  ]\n",
      " [-0.00740667 -0.07764267 -0.18821    ... -0.06809667  0.11845332\n",
      "  -0.25808   ]\n",
      " [ 0.23648     0.39091    -0.095802   ...  0.13228     0.36417\n",
      "  -0.28676   ]]\n"
     ]
    }
   ],
   "source": [
    "# Prepare for a tensorboard visualization\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "output_init = tf.constant_initializer(output)\n",
    "\n",
    "print('fitting shape:')\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() :\n",
    "    embedding_var = tf.get_variable('embedding_var', shape=[len(output), len(output[0])], initializer=tf.constant_initializer(output), dtype=tf.float32)\n",
    "    embedding_var.initializer.run()\n",
    "    print(embedding_var.eval())\n",
    "    \n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(embedding_var.initializer)\n",
    "\n",
    "\n",
    "with open(path_for_tf_ckpt,'w') as f:\n",
    "    f.write(\"Index\\tLabel\\n\")\n",
    "    for ind,label_line in enumerate(index):\n",
    "        label = '{}:{}:{}'.format(label_line[0], label_line[1], label_line[2])\n",
    "        f.write(\"%d\\t%s\\n\" % (ind,label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the tensorboard embedding visualization\n",
    "# To view it, run command \"tensorboard --port=6006 --logdir=./logdir\" on your computer and then \n",
    "# open http://localhost:6006 in a browser.\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # Create summary writer.\n",
    "    writer = tf.summary.FileWriter(path_for_tf_metadata, sess.graph)\n",
    "    # Initialize embedding_var\n",
    "    sess.run(embedding_var.initializer)\n",
    "    # Create Projector config\n",
    "    config = projector.ProjectorConfig()\n",
    "    # Add embedding visualizer\n",
    "    embedding = config.embeddings.add()\n",
    "    # Attache the name 'embedding'\n",
    "    embedding.tensor_name = embedding_var.name\n",
    "    # Metafile which is described later\n",
    "    embedding.metadata_path = 'metadata.tsv'\n",
    "    # Add writer and config to Projector\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "    # Save the model\n",
    "    saver_embed = tf.train.Saver([embedding_var])\n",
    "    saver_embed.save(sess, path_for_tf_ckpt, 1)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_vectors(text, nlp, ignore_words, lemmas):\n",
    "    \"\"\" Preprocess test text content, for predicting labels.\"\"\"\n",
    "    \n",
    "    sample_index = []\n",
    "    sample_vectors = None\n",
    "    \n",
    "    for concept_vec in get_vectors(text, nlp, ignore_words, lemmas):\n",
    "            \n",
    "        # Append data to a list and a numpy array\n",
    "        sample_index.append([product, rating, concept_vec[0]])\n",
    "        \n",
    "        if sample_vectors is None:\n",
    "            # Create an np.array with the first row as the retrieved word vector\n",
    "            sample_vectors = np.array([concept_vec[1]])\n",
    "        else:\n",
    "            # Append the next vector to the end of the vectors array\n",
    "            sample_vectors = np.append(sample_vectors, np.array([concept_vec[1]]), axis=0)            \n",
    "    \n",
    "    return sample_index, sample_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_labels(cluster_ind):\n",
    "    sample_labels = set()\n",
    "    for word_ind in exemplars(clusters[cluster_ind], tree):\n",
    "        sample_labels.add(index[word_ind][2])\n",
    "        \n",
    "    return sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_labels:  [ 1 90 -1 19 -1 59 -1 -1 31 -1 -1]\n",
      "Phrase 'i' is predicted to have labels {'i'}.\n",
      "Phrase 'it' is predicted to have labels {'it'}.\n",
      "Phrase 'children' is predicted to have labels {'children'}.\n",
      "Phrase 'whole family' is predicted to have labels {'original version', 'addams family', 'family', 'whole family', 'old version', 'family version', 'version'}.\n",
      "Phrase 'something' is predicted to have labels {'something'}.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"I would buy this again. It was a very good deal and good for children 2-4. Usually, this is good fun and good for the whole family, brothers, sisters and friends. Another with a magnetic personality was buying sweets for his sweet. Something should be done to handle all shapes and sizes and enable then to be put together.\"\n",
    "\n",
    "sample_index, sample_vectors = generate_sample_vectors(sample_text, nlp, ignore_words, lemmas)\n",
    "\n",
    "test_labels, strengths = hdbscan.approximate_predict(hdbscanner, sample_vectors)\n",
    "print(\"test_labels: \",test_labels)\n",
    "\n",
    "for ind, word_index in enumerate(sample_index):\n",
    "    if (test_labels[ind] >0):\n",
    "        print(\"Phrase '{}' is predicted to have labels {}.\".format(word_index[2], get_sample_labels(test_labels[ind])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spacy3.5]",
   "language": "python",
   "name": "conda-env-spacy3.5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
