{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example of attention layer use\n",
    "\n",
    "### Train RNN (GRU) + Attention on IMDB dataset (binary classification)\n",
    "#### Learning and hyper-parameters were not tuned; script serves as an example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip install numpy\n",
    "# !sudo pip install tensorflow\n",
    "# !sudo pip install keras\n",
    "# !sudo pip install tqdmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention import attention\n",
    "from rnn_common.utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 32768\n",
    "INDEX_FROM = 3\n",
    "SKIP_TOP = 10 # This skips the 10 most frequent words in the IMDB reviews text\n",
    "SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 150\n",
    "ATTENTION_SIZE = 50\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "MODEL_PATH = './model'\n",
    "train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Replace imdb.load_data with load of Amazon product review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM, skip_top=SKIP_TOP)\n",
    "\n",
    "# Sequences pre-processing\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different placeholders\n",
    "with tf.name_scope('Inputs'):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "with tf.name_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-06f3ed5d6bb5>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-7-06f3ed5d6bb5>:3: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_outputs:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Bi-)RNN layer(-s)\n",
    "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "tf.summary.histogram('RNN_outputs', rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-9e68b1111956>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Attention layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    tf.summary.histogram('alphas', alphas)\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(attention_output, keep_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "with tf.name_scope('Fully_connected_layer'):\n",
    "    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
    "    b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    tf.summary.histogram('W', W)\n",
    "\n",
    "with tf.name_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Batch generators\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "epoch: 0\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [02:37<00:00,  1.61s/it]\n",
      "100%|██████████| 97/97 [01:43<00:00,  1.07s/it]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.445, val_loss: 0.437, acc: 0.708, val_acc: 0.795\n",
      "epoch: 1\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [02:35<00:00,  1.59s/it]\n",
      "100%|██████████| 97/97 [01:43<00:00,  1.06s/it]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.307, val_loss: 0.364, acc: 0.844, val_acc: 0.841\n",
      "epoch: 2\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [02:41<00:00,  1.78s/it]\n",
      "100%|██████████| 97/97 [01:43<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.258, val_loss: 0.336, acc: 0.897, val_acc: 0.855\n",
      "Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "\n",
      "Following are words with comparative attention scores and visualization with color background.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font style=\"background: rgba(255, 255, 0, 0.404966)\" size=2.809932>:UNK: (0.404966)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.136318)\" size=2.272635>please (0.136318)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.117125)\" size=2.234250>give (0.117125)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.613737)\" size=3.227474>this (0.613737)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.248684)\" size=2.497368>one (0.248684)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.983943)\" size=3.967885>:UNK: (0.983943)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.465721)\" size=2.931442>miss (0.465721)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.360551)\" size=2.721103>br (0.360551)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.164061)\" size=2.328123>br (0.164061)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.121583)\" size=2.243167>kristy (0.121583)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.181577)\" size=2.363155>swanson (0.181577)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.527144)\" size=3.054288>:UNK: (0.527144)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.685913)\" size=3.371825>:UNK: (0.685913)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.649333)\" size=3.298666>rest (0.649333)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 1.000000)\" size=4.000000>:UNK: (1.000000)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.901851)\" size=3.803703>:UNK: (0.901851)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.582946)\" size=3.165892>cast (0.582946)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.623750)\" size=3.247500>rendered (0.623750)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.408788)\" size=2.817577>terrible (0.408788)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.263534)\" size=2.527069>performances (0.263534)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.496962)\" size=2.993924>:UNK: (0.496962)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.344413)\" size=2.688827>show (0.344413)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.454081)\" size=2.908162>:UNK: (0.454081)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.064202)\" size=2.128403>flat (0.064202)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.020212)\" size=2.040424>flat (0.020212)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.007610)\" size=2.015220>flat (0.007610)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.003192)\" size=2.006383>br (0.003192)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.000935)\" size=2.001871>br (0.000935)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.000564)\" size=2.001127>i (0.000564)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.000499)\" size=2.000999>don't (0.000499)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.000600)\" size=2.001200>know (0.000600)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.000501)\" size=2.001002>how (0.000501)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.000918)\" size=2.001835>michael (0.000918)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.001632)\" size=2.003264>madison (0.001632)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.001414)\" size=2.002829>could (0.001414)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.001253)\" size=2.002506>have (0.001253)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.001532)\" size=2.003064>allowed (0.001532)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.014308)\" size=2.028616>this (0.014308)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.002036)\" size=2.004071>one (0.002036)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.004736)\" size=2.009471>on (0.004736)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.005864)\" size=2.011729>his (0.005864)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.006896)\" size=2.013791>plate (0.006896)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.005399)\" size=2.010798>he (0.005399)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.013217)\" size=2.026433>almost (0.013217)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.035685)\" size=2.071370>seemed (0.035685)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.137649)\" size=2.275299>:UNK: (0.137649)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.055009)\" size=2.110018>know (0.055009)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.248261)\" size=2.496521>this (0.248261)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.087926)\" size=2.175852>wasn't (0.087926)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.077229)\" size=2.154457>going (0.077229)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.131556)\" size=2.263112>:UNK: (0.131556)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.056714)\" size=2.113427>work (0.056714)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.082131)\" size=2.164261>out (0.082131)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.169081)\" size=2.338162>:UNK: (0.169081)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.048262)\" size=2.096523>his (0.048262)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.081846)\" size=2.163693>performance (0.081846)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.040059)\" size=2.080117>was (0.040059)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.075441)\" size=2.150883>quite (0.075441)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.046805)\" size=2.093610>lacklustre (0.046805)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.019857)\" size=2.039715>so (0.019857)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.025301)\" size=2.050602>all (0.025301)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.009614)\" size=2.019227>you (0.009614)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.043992)\" size=2.087985>madison (0.043992)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.022196)\" size=2.044392>fans (0.022196)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.048537)\" size=2.097073>give (0.048537)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.293289)\" size=2.586579>this (0.293289)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.284831)\" size=2.569662>:UNK: (0.284831)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.054457)\" size=2.108915>miss (0.054457)<br></font>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Calculate alpha coefficients for the first test example\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "\n",
    "    x_batch_test, y_batch_test = X_test[:], y_test[:]\n",
    "    seq_len_test = np.array([list(x).index(0) + 1 for x in x_batch_test])\n",
    "    alphas_test = sess.run([alphas], feed_dict={batch_ph: x_batch_test, target_ph: y_batch_test,\n",
    "                                                seq_len_ph: seq_len_test, keep_prob_ph: 1.0})\n",
    "alphas_values = alphas_test[0][0]\n",
    "\n",
    "# Build correct mapping from word to index and inverse\n",
    "word_index = imdb.get_word_index()\n",
    "word_index = {word: index + INDEX_FROM for word, index in word_index.items()}\n",
    "word_index[\":PAD:\"] = 0\n",
    "word_index[\":START:\"] = 1\n",
    "word_index[\":UNK:\"] = 2\n",
    "index_word = {value: key for key, value in word_index.items()}\n",
    "# Represent the sample by words rather than indices\n",
    "words = list(map(index_word.get, x_batch_test[0]))\n",
    "\n",
    "# Save visualization as HTML\n",
    "with open(\"visualization.html\", \"w\") as html_file:\n",
    "    for word, alpha in zip(words, alphas_values / alphas_values.max()):\n",
    "        if word == \":START:\":\n",
    "            continue\n",
    "        elif word == \":PAD:\":\n",
    "            break\n",
    "        html_file.write('<font style=\"background: rgba(255, 255, 0, %f)\" size=%f>%s (%f)<br></font>\\n' % (alpha, 2+alpha*2, word, alpha))\n",
    "\n",
    "print('\\nFollowing are words with comparative attention scores and visualization with color background.')\n",
    "\n",
    "with open(\"visualization.html\", \"r\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "\n",
    "HTML(filename=\"./visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
