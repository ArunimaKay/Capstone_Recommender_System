{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example of attention layer use\n",
    "\n",
    "### Train RNN (GRU) + Attention on IMDB dataset (binary classification)\n",
    "#### Learning and hyper-parameters were not tuned; script serves as an example \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip install numpy\n",
    "# !sudo pip install tensorflow\n",
    "# !sudo pip install keras\n",
    "# !sudo pip install tqdmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention import attention\n",
    "from rnn_common.utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 10000\n",
    "INDEX_FROM = 3\n",
    "SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 150\n",
    "ATTENTION_SIZE = 50\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "MODEL_PATH = './model'\n",
    "train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Replace imdb.load_data with load of Amazon view dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "\n",
    "# Sequences pre-processing\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different placeholders\n",
    "with tf.name_scope('Inputs'):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "with tf.name_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-06f3ed5d6bb5>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-06f3ed5d6bb5>:3: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_outputs:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Bi-)RNN layer(-s)\n",
    "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "tf.summary.histogram('RNN_outputs', rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-9e68b1111956>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Attention layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    tf.summary.histogram('alphas', alphas)\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(attention_output, keep_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "with tf.name_scope('Fully_connected_layer'):\n",
    "    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
    "    b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    tf.summary.histogram('W', W)\n",
    "\n",
    "with tf.name_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Batch generators\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "epoch: 0\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [02:46<00:00,  1.71s/it]\n",
      "100%|██████████| 97/97 [01:43<00:00,  1.06s/it]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.447, val_loss: 0.426, acc: 0.715, val_acc: 0.805\n",
      "epoch: 1\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [02:43<00:00,  1.71s/it]\n",
      "100%|██████████| 97/97 [01:42<00:00,  1.04s/it]\n",
      "  0%|          | 0/97 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.303, val_loss: 0.346, acc: 0.844, val_acc: 0.849\n",
      "epoch: 2\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [02:42<00:00,  1.70s/it]\n",
      "100%|██████████| 97/97 [1:21:13<00:00,  5.02s/it]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.256, val_loss: 0.318, acc: 0.891, val_acc: 0.864\n",
      "Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\n"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./model\n",
      "\n",
      "Following are words with comparative attention scores and visualization with color background.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font style=\"background: rgba(255, 255, 0, 0.470729)\">please (0.470729)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.470103)\">give (0.470103)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.758374)\">this (0.758374)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.794364)\">one (0.794364)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 1.000000)\">a (1.000000)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.610672)\">miss (0.610672)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.766846)\">br (0.766846)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.940234)\">br (0.940234)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.677752)\">:UNK: (0.677752)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.473367)\">:UNK: (0.473367)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.744163)\">and (0.744163)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.737714)\">the (0.737714)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.944222)\">rest (0.944222)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.523225)\">of (0.523225)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.457614)\">the (0.457614)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.368004)\">cast (0.368004)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.245767)\">rendered (0.245767)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.470172)\">terrible (0.470172)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.289606)\">performances (0.289606)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.423774)\">the (0.423774)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.330396)\">show (0.330396)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.642559)\">is (0.642559)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.443916)\">flat (0.443916)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.353467)\">flat (0.353467)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.263812)\">flat (0.263812)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.209884)\">br (0.209884)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.201693)\">br (0.201693)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.102317)\">i (0.102317)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.112516)\">don't (0.112516)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.109850)\">know (0.109850)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.145045)\">how (0.145045)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.074107)\">michael (0.074107)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.033790)\">madison (0.033790)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.025448)\">could (0.025448)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.030228)\">have (0.030228)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.018262)\">allowed (0.018262)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.029915)\">this (0.029915)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.023066)\">one (0.023066)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.008527)\">on (0.008527)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.015202)\">his (0.015202)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.039171)\">plate (0.039171)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.041274)\">he (0.041274)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.080787)\">almost (0.080787)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.028768)\">seemed (0.028768)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.029080)\">to (0.029080)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.027637)\">know (0.027637)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.073499)\">this (0.073499)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.080297)\">wasn't (0.080297)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.061540)\">going (0.061540)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.036546)\">to (0.036546)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.046670)\">work (0.046670)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.045530)\">out (0.045530)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.125463)\">and (0.125463)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.076067)\">his (0.076067)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.082874)\">performance (0.082874)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.245774)\">was (0.245774)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.217262)\">quite (0.217262)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.128153)\">:UNK: (0.128153)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.093987)\">so (0.093987)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.337191)\">all (0.337191)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.175428)\">you (0.175428)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.161172)\">madison (0.161172)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.118088)\">fans (0.118088)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.212923)\">give (0.212923)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.394923)\">this (0.394923)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.441935)\">a (0.441935)<br></font>\n",
       "<font style=\"background: rgba(255, 255, 0, 0.265215)\">miss (0.265215)<br></font>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Calculate alpha coefficients for the first test example\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "\n",
    "    x_batch_test, y_batch_test = X_test[:10], y_test[:10]\n",
    "    seq_len_test = np.array([list(x).index(0) + 1 for x in x_batch_test])\n",
    "    alphas_test = sess.run([alphas], feed_dict={batch_ph: x_batch_test, target_ph: y_batch_test,\n",
    "                                                seq_len_ph: seq_len_test, keep_prob_ph: 1.0})\n",
    "alphas_values = alphas_test[0][0]\n",
    "\n",
    "# Build correct mapping from word to index and inverse\n",
    "word_index = imdb.get_word_index()\n",
    "word_index = {word: index + INDEX_FROM for word, index in word_index.items()}\n",
    "word_index[\":PAD:\"] = 0\n",
    "word_index[\":START:\"] = 1\n",
    "word_index[\":UNK:\"] = 2\n",
    "index_word = {value: key for key, value in word_index.items()}\n",
    "# Represent the sample by words rather than indices\n",
    "words = list(map(index_word.get, x_batch_test[0]))\n",
    "\n",
    "# Save visualization as HTML\n",
    "with open(\"visualization.html\", \"w\") as html_file:\n",
    "    for word, alpha in zip(words, alphas_values / alphas_values.max()):\n",
    "        if word == \":START:\":\n",
    "            continue\n",
    "        elif word == \":PAD:\":\n",
    "            break\n",
    "        html_file.write('<font style=\"background: rgba(255, 255, 0, %f)\">%s (%f)<br></font>\\n' % (alpha, word, alpha))\n",
    "\n",
    "print('\\nFollowing are words with comparative attention scores and visualization with color background.')\n",
    "\n",
    "with open(\"visualization.html\", \"r\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "\n",
    "HTML(filename=\"./visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
