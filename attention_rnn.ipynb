{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train RNN (GRU) + Attention on Amason product review/rating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip install numpy\n",
    "# !sudo pip install tensorflow\n",
    "# !sudo pip install keras\n",
    "# !sudo pip install tqdmn\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "import gzip\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention import attention\n",
    "from rnn_common.utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = -1\n",
    "INDEX_FROM = 3\n",
    "# SKIP_TOP = 0 # This will instead be handled by filtering stop words prior to training/testing\n",
    "SEQUENCE_LENGTH = 50 # The mean sequence length in training data is 32 (after filtering stop words)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 200\n",
    "ATTENTION_SIZE = SEQUENCE_LENGTH * 2\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "MODEL_PATH = './model'\n",
    "FILE_PATH = './data/reviews_Toys_and_Games_5.json.gz'\n",
    "UNKNOWN_WORDS_FILE_PATH = './data/unknown_words.txt'\n",
    "PAD_ID = 0\n",
    "START_ID = 1\n",
    "UNK_ID = 2\n",
    "\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_lines(filepath):\n",
    "    line_count = 0\n",
    "    with gzip.open(filepath) as input_file:\n",
    "        for line in input_file:\n",
    "            line_count += 1            \n",
    "    return line_count\n",
    "            \n",
    "\n",
    "def get_splits(filepath, split, split_add_words=None):\n",
    "    if sum(split) != 1.0:\n",
    "        raise ValueError(f\"Error, split items {split} do not add up to 1.\")\n",
    "        \n",
    "    # Default to adding words only from the train dataset\n",
    "    if split_add_words is None:\n",
    "        split_add_words = [False*len(split)]\n",
    "        split_add_words[0] = True\n",
    "        \n",
    "    vocab = {}\n",
    "    debug = False\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    print(f\"Filtering ({len(stopWords)}) English stop words)\")\n",
    "    \n",
    "    X_datasets = []\n",
    "    y_datasets = []\n",
    "    \n",
    "    # populate empty value lists for each split\n",
    "    for _ in range(len(split)):\n",
    "        X_datasets.append([])\n",
    "        y_datasets.append([])\n",
    "    \n",
    "    line_count = num_lines(filepath)\n",
    "    \n",
    "    split_arr = []\n",
    "    for ind in range(len(split)):\n",
    "        # append list of [0]s, [1]s, etc. each of the size indicated by split\n",
    "        item_count = int(split[ind]*line_count) + 1\n",
    "        print(f\"Appending {item_count} [{ind}]s for split {split[ind]}...\")\n",
    "        split_arr.extend([int(ind)]*item_count)\n",
    "        \n",
    "    split_arr = np.array(split_arr)\n",
    "    \n",
    "    # Shuffle the split inds just added\n",
    "    np.random.shuffle(split_arr)\n",
    "    \n",
    "    print(f\"Shuffled split_arr: {split_arr}\")\n",
    "    \n",
    "    ## unknown words file exists, delete it\n",
    "    if os.path.isfile(UNKNOWN_WORDS_FILE_PATH):\n",
    "        os.remove(UNKNOWN_WORDS_FILE_PATH)\n",
    "\n",
    "    with gzip.open(filepath) as input_file, open(UNKNOWN_WORDS_FILE_PATH, 'a') as unknown_file:\n",
    "        curr_line = 0\n",
    "        for line in input_file:\n",
    "            # Look back into the split list shuffled above to determine the appropriate split for this line\n",
    "            split_ind = split_arr[curr_line]\n",
    "            \n",
    "            if curr_line < 10:\n",
    "                print(f\"\\nLine {curr_line}:\", line)\n",
    "            else:\n",
    "                # Only enable debug output for at most the first 10 lines\n",
    "                debug = False\n",
    "                \n",
    "            # Add the review text to the appropriate split, only adding to the vocab as specified in split_add_words.\n",
    "            # Default behavior is to add words to vocab only for the first split (train)\n",
    "            word_list = get_values(line, 'reviewText', vocab=vocab, stopwords=stopWords,\n",
    "                                   add_words=(split_add_words[split_ind]), debug=debug, unknown_file=unknown_file)\n",
    "            if len(word_list) > 0:\n",
    "                X_datasets[split_ind].append(word_list)\n",
    "                y_datasets[split_ind].append(get_values(line, 'overall', debug=debug))\n",
    "                if debug:\n",
    "                    print(f\"split_ind: {split_ind}, curr_line: {curr_line}\")\n",
    "                    split_rec = len(X_datasets[split_ind])-1\n",
    "                    print(f\"X_datasets[{split_ind}][{split_rec}]: {X_datasets[split_ind][split_rec]}\")\n",
    "                    print(f\"y_datasets[{split_ind}][{split_rec}]: {y_datasets[split_ind][split_rec]}\")\n",
    "            \n",
    "            curr_line += 1\n",
    "                                        \n",
    "    print(\"Processed input file to produce the following randomized splits:\")\n",
    "    for ind in range(len(X_datasets)):\n",
    "        print(f\"[{ind}] : {len(X_datasets[ind])} samples\")\n",
    "        \n",
    "    return X_datasets, y_datasets, vocab\n",
    "\n",
    "    \n",
    "    \n",
    "def get_values(line, column, vocab=None, stopwords=None, add_words=False, debug=False, unknown_file=None):\n",
    "\n",
    "    line_dict = eval(line)\n",
    "    line_val = line_dict[column]\n",
    "    \n",
    "    \n",
    "    if vocab is not None:\n",
    "        line_ids = []\n",
    "        for word in line_val.lower().split(\" \"):\n",
    "            if (stopwords is not None) and (word in stopwords):\n",
    "                # skip stop words\n",
    "                continue\n",
    "                \n",
    "            word_id = vocab.get(word)\n",
    "            if word_id is None:\n",
    "                if add_words and ((NUM_WORDS<0) or (len(vocab)<(NUM_WORDS-1))):\n",
    "                    # Create a new word_id\n",
    "                    word_id = len(vocab)\n",
    "                    vocab[word] = word_id\n",
    "                    if debug:\n",
    "                        print(f\"Added word {word} with id {word_id}\")\n",
    "                else:\n",
    "                    # In the test dataset, unknown words will be skipped, but written to unknown file for review\n",
    "                    if unknown_file is not None:\n",
    "                        unknown_file.write(word)\n",
    "                    continue\n",
    "\n",
    "            line_ids.append(word_id)\n",
    "            if debug:\n",
    "                print(f\"IDS: {line_ids}\")\n",
    "                \n",
    "        # Start a word_id list with the actual seq_len\n",
    "        seq_len = [len(line_ids)]\n",
    "        line_val = line_ids\n",
    "        if debug:\n",
    "            print(f\"seq_len: {seq_len}\")\n",
    "            print(f\"line_val: {line_val}\")\n",
    "        \n",
    "        # For text values, the list of word ids will be returned\n",
    "        return line_ids\n",
    "    else:\n",
    "        # For the y values, an binarized int value (converted from float) will be returned\n",
    "        return int(line_val/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading inputs...\n",
      "Filtering (179) English stop words)\n",
      "Appending 150838 [0]s for split 0.9...\n",
      "Appending 16760 [1]s for split 0.1...\n",
      "Shuffled split_arr: [0 0 0 ... 0 1 0]\n",
      "\n",
      "Line 0: b'{\"reviewerID\": \"A1VXOAVRGKGEAK\", \"asin\": \"0439893577\", \"reviewerName\": \"Angie\", \"helpful\": [0, 0], \"reviewText\": \"I like the item pricing. My granddaughter wanted to mark on it but I wanted it just for the letters.\", \"overall\": 5.0, \"summary\": \"Magnetic board\", \"unixReviewTime\": 1390953600, \"reviewTime\": \"01 29, 2014\"}\\n'\n",
      "\n",
      "Line 1: b'{\"reviewerID\": \"A8R62G708TSCM\", \"asin\": \"0439893577\", \"reviewerName\": \"Candace\", \"helpful\": [1, 1], \"reviewText\": \"Love the magnet easel... great for moving to different areas... Wish it had some sort of non skid pad on bottom though...\", \"overall\": 4.0, \"summary\": \"it works pretty good for moving to different areas\", \"unixReviewTime\": 1395964800, \"reviewTime\": \"03 28, 2014\"}\\n'\n",
      "\n",
      "Line 2: b'{\"reviewerID\": \"A21KH420DK0ICA\", \"asin\": \"0439893577\", \"reviewerName\": \"capemaychristy\", \"helpful\": [1, 1], \"reviewText\": \"Both sides are magnetic.  A real plus when you\\'re entertaining more than one child.  The four-year old can find the letters for the words, while the two-year old can find the pictures the words spell.  (I bought letters and magnetic pictures to go with this board).  Both grandkids liked it a lot, which means I like it a lot as well.  Have not even introduced markers, as this will be used strictly as a magnetic board.\", \"overall\": 5.0, \"summary\": \"love this!\", \"unixReviewTime\": 1359331200, \"reviewTime\": \"01 28, 2013\"}\\n'\n",
      "\n",
      "Line 3: b'{\"reviewerID\": \"AR29QK6HPFYZ4\", \"asin\": \"0439893577\", \"reviewerName\": \"dcrm\", \"helpful\": [0, 0], \"reviewText\": \"Bought one a few years ago for my daughter and she loves it, still using it today. For the holidays we bought one for our niece and she loved it too.\", \"overall\": 5.0, \"summary\": \"Daughters love it\", \"unixReviewTime\": 1391817600, \"reviewTime\": \"02 8, 2014\"}\\n'\n",
      "\n",
      "Line 4: b'{\"reviewerID\": \"ACCH8EOML6FN5\", \"asin\": \"0439893577\", \"reviewerName\": \"DoyZ\", \"helpful\": [1, 1], \"reviewText\": \"I have a stainless steel refrigerator therefore there are not much space for my son to play with his magnet. Brought this for him to put his magnet on. He enjoys sticking his magnet on it. Great to have so he can play with his alphabet magnets.\", \"overall\": 4.0, \"summary\": \"Great to have so he can play with his alphabet ...\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"}\\n'\n",
      "\n",
      "Line 5: b'{\"reviewerID\": \"A1EDPGHC9EYBUN\", \"asin\": \"0439893577\", \"reviewerName\": \"kp\", \"helpful\": [0, 0], \"reviewText\": \"this is a nice magnetic board for the kids to carry around and play with the letter bought additional magnetic pieces from mellissa and doug to increase thier play time with them hope it works\", \"overall\": 3.0, \"summary\": \"grandma\", \"unixReviewTime\": 1368662400, \"reviewTime\": \"05 16, 2013\"}\\n'\n",
      "\n",
      "Line 6: b'{\"reviewerID\": \"A181ZNB42LISRZ\", \"asin\": \"0439893577\", \"reviewerName\": \"laura\", \"helpful\": [0, 0], \"reviewText\": \"This is just as I expected. It\\'s not too sturdy but it stands up well. I\\'m not sure how long it will last, but my grandsons love plalying with it.\", \"overall\": 3.0, \"summary\": \"Serves the purpose\", \"unixReviewTime\": 1356480000, \"reviewTime\": \"12 26, 2012\"}\\n'\n",
      "\n",
      "Line 7: b'{\"reviewerID\": \"A1RBEJ5UQ1RWAW\", \"asin\": \"0439893577\", \"reviewerName\": \"Linda\", \"helpful\": [1, 1], \"reviewText\": \"My granddaughter really really likes this. I love that you can just fold it up and put it away. Would definately recommend.\", \"overall\": 5.0, \"summary\": \"Great\", \"unixReviewTime\": 1397520000, \"reviewTime\": \"04 15, 2014\"}\\n'\n",
      "\n",
      "Line 8: b'{\"reviewerID\": \"AFGGC7SIV964O\", \"asin\": \"0439893577\", \"reviewerName\": \"Maan\\'s Mom\", \"helpful\": [0, 0], \"reviewText\": \"This is good product to have with unlimited possibilities. I also ordered the magnetic letters & numbers by Melissa & Doug.  The dry erase has to be erased quickly or they tend to leave marks. Also, the magnet is not very strong. It\\'s a nice size & works very well for our needs.\", \"overall\": 4.0, \"summary\": \"A must have for any homeschooling classroom !\", \"unixReviewTime\": 1364428800, \"reviewTime\": \"03 28, 2013\"}\\n'\n",
      "\n",
      "Line 9: b'{\"reviewerID\": \"A2XU46XXNV19C8\", \"asin\": \"0439893577\", \"reviewerName\": \"Margaret\", \"helpful\": [59, 59], \"reviewText\": \"I keep this board on top of the hallway table so that I can quickly write notes (which won\\'t get lost until I erase them) and it folds down neatly so it\\'s easy to hide when company calls.  I also like the size -- not too big and not too small.  Quality magnets hold pretty good but I use it mainly for notes. NOTE:  Children\\'s alphabet letters don\\'t hold very well - they tend to slide.\", \"overall\": 3.0, \"summary\": \"Nifty little thing\", \"unixReviewTime\": 1258588800, \"reviewTime\": \"11 19, 2009\"}\\n'\n",
      "Processed input file to produce the following randomized splits:\n",
      "[0] : 150837 samples\n",
      "[1] : 16760 samples\n",
      "...finished reading input file, after 18 seconds\n",
      "Results:\n",
      "    X_datasets : 2 elements\n",
      "\n",
      "X_train (150837 records): [[0, 1, 2, 3, 4, 5, 4, 6], [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51, 52, 53, 40, 54], [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39, 27, 64, 65, 66], [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81, 10, 74, 82, 83]]\n",
      "y_train (150837 records): [1, 0, 1, 1, 0]\n",
      "Percentiles (25%, 50%, 75%, 100%) review_length (X_train) = [  18.   33.   65. 3168.]\n",
      "\n",
      "X_test (16760 records): [[48, 7, 81], [85, 120, 118, 23, 111, 0, 126, 23, 107, 23, 82, 32, 127, 121], [39, 85, 115, 93, 40, 32, 114, 23, 126, 10, 74, 21, 23, 66, 23, 111, 39, 40, 32, 23, 111, 23, 126], [35, 289, 407, 23, 216, 399, 245], [39, 289, 73, 320, 91, 156, 145, 289, 76, 145, 191, 470, 341]]\n",
      "y_test (16760 records): [0, 0, 1, 0, 0]\n",
      "\n",
      "vocab contains 339415 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReading inputs...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split inputs into X_train and X_test. X_train will add to vocabulary, and X_test will not.\n",
    "X_datasets, y_datasets, vocab = get_splits(FILE_PATH, [.9,.1], [True, False])\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"...finished reading input file, after {round(elapsed)} seconds\")\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"    X_datasets : {len(X_datasets)} elements\")\n",
    "\n",
    "X_train = X_datasets[0]\n",
    "y_train = y_datasets[0]\n",
    "print(f\"\\nX_train ({len(X_train)} records): {X_train[:5]}\")\n",
    "print(f\"y_train ({len(y_train)} records): {y_train[:5]}\")\n",
    "\n",
    "percents_review_len = np.percentile([len(review) for review in X_train], [25,50,75,100])\n",
    "print(f\"Percentiles (25%, 50%, 75%, 100%) review_length (X_train) = {percents_review_len}\")\n",
    "\n",
    "X_test = X_datasets[1]\n",
    "y_test = y_datasets[1]\n",
    "print(f\"\\nX_test ({len(X_test)} records): {X_test[:5]}\")\n",
    "print(f\"y_test ({len(y_test)} records): {y_test[:5]}\")\n",
    "\n",
    "print(f\"\\nvocab contains {len(vocab)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 4, 6], [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]]\n",
      "0 entries in X_train have length 0\n"
     ]
    }
   ],
   "source": [
    "# Sequences pre-processing\n",
    "print(X_train[:2][:5])\n",
    "print(f\"{sum(1 for x in X_train if ((len(x) == 0) or (x is None)))} entries in X_train have length 0\")\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different placeholders\n",
    "with tf.name_scope('Inputs'):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "with tf.name_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([len(vocab)+1, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-06f3ed5d6bb5>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-06f3ed5d6bb5>:3: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_outputs:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Bi-)RNN layer(-s)\n",
    "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "tf.summary.histogram('RNN_outputs', rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-9e68b1111956>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Attention layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    tf.summary.histogram('alphas', alphas)\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(attention_output, keep_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "with tf.name_scope('Fully_connected_layer'):\n",
    "    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
    "    b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    tf.summary.histogram('W', W)\n",
    "\n",
    "with tf.name_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-96966693c0ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./logdir/test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msession_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_growth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_soft_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_device_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Options'"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Batch generators\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(_options=tf.Options(allow_growth=True),allow_soft_placement=True, log_device_placement=True)\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "\n",
    "            print(\"\\n*********************************************\")\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            print(\"\\nTraining...\")\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            print(\"\\nTesting with unknown words...\")\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\\n\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "vocab_ind = {}\n",
    "for word,ind in vocab.items():\n",
    "    vocab_ind[ind]=word\n",
    "    \n",
    "print(f\"Just created id->word dict of size {len(vocab_ind)} items\")\n",
    "print(f\" .   max(ind) = {max(vocab_ind.keys())}\")\n",
    "print(f\"Source was word->id dict of size {len(vocab)} items\")\n",
    "print(f\" .   max(ind) = {max(vocab.values())}\")\n",
    "              \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Calculate alpha coefficients for the first test example\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "\n",
    "    x_batch_test, y_batch_test = X_test[:], y_test[:]\n",
    "    seq_len_test = np.array([list(x).index(0) + 1 for x in x_batch_test])\n",
    "    alphas_test = sess.run([alphas], feed_dict={batch_ph: x_batch_test, target_ph: y_batch_test,\n",
    "                                                seq_len_ph: seq_len_test, keep_prob_ph: 1.0})\n",
    "alphas_values = alphas_test[0][0]\n",
    "\n",
    "# Build correct mapping from word to index and inverse\n",
    "word_index = vocab\n",
    "word_index = {word: index + INDEX_FROM for word, index in word_index.items()}\n",
    "word_index[\":PAD:\"] = PAD_ID\n",
    "word_index[\":START:\"] = START_ID\n",
    "word_index[\":UNK:\"] = UNK_ID\n",
    "index_word = {value: key for key, value in word_index.items()}\n",
    "# Represent the sample by words rather than indices\n",
    "words = list(map(index_word.get, x_batch_test[0]))\n",
    "\n",
    "# Save visualization as HTML\n",
    "with open(\"visualization.html\", \"w\") as html_file:\n",
    "    for word, alpha in sorted(zip(words, alphas_values / alphas_values.max()), \n",
    "                              key=lambda entry: -entry[1]):\n",
    "        if word == \":START:\":\n",
    "            continue\n",
    "        elif word == \":PAD:\":\n",
    "            break\n",
    "        html_file.write('<font style=\"background: rgba(255, 255, 0, %f)\" size=%f>%s (%f)<br></font>\\n' % (alpha, 2+alpha*2, word, alpha))\n",
    "\n",
    "print('\\nFollowing are words with comparative attention scores and visualization with color background.')\n",
    "\n",
    "with open(\"visualization.html\", \"r\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "\n",
    "HTML(filename=\"./visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
