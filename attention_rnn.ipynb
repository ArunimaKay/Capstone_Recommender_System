{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of attention layer use\n",
    "\n",
    "### Train RNN (GRU) + Attention on Amason product review/rating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip install numpy\n",
    "# !sudo pip install tensorflow\n",
    "# !sudo pip install keras\n",
    "# !sudo pip install tqdmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "import gzip\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention import attention\n",
    "from rnn_common.utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 40000\n",
    "INDEX_FROM = 3\n",
    "SKIP_TOP = 10 # This skips the 10 most frequent words in the IMDB reviews text\n",
    "SEQUENCE_LENGTH = 250\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 150\n",
    "ATTENTION_SIZE = 50\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "MODEL_PATH = './model'\n",
    "FILE_PATH = './data/reviews_Toys_and_Games_5.json.gz'\n",
    "train = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Replace imdb.load_data with load of Amazon product review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_lines(filepath):\n",
    "    line_count = 0\n",
    "    with gzip.open(filepath) as input_file:\n",
    "        for line in input_file:\n",
    "            line_count += 1            \n",
    "    return line_count\n",
    "            \n",
    "\n",
    "def get_values(filepath, column, bounds, vocab=None):\n",
    "    line_count = num_lines(filepath)\n",
    "    start_line = bounds[0] * line_count\n",
    "    end_line = bounds[1] * line_count\n",
    "\n",
    "    with gzip.open(filepath) as input_file:\n",
    "        curr_line = 0\n",
    "        for line in input_file:\n",
    "            curr_line += 1\n",
    "            if curr_line >= start_line and curr_line < end_line:\n",
    "                line_dict = eval(line)\n",
    "                line_val = line_dict[column]\n",
    "                \n",
    "                if vocab is not None:\n",
    "                    line_ids = []\n",
    "                    for word in line_val.lower().split(\" \"):\n",
    "                        word_id = vocab.get(word)\n",
    "                        if word_id is None:\n",
    "                            word_id = len(vocab)+4\n",
    "                            vocab[word] = word_id\n",
    "                        line_ids.append(word_id)\n",
    "                    line_val = line_ids\n",
    "                \n",
    "                yield line_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading train input...\n",
      "...finished reading train input, after 15 seconds\n",
      "X_train (134077 records): [[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 4, 11, 15, 17, 18, 6, 19], [20, 6, 21, 22, 23, 18, 24, 12, 25, 26, 27, 15, 28, 29, 30, 31, 32, 33, 34, 14, 35, 36]]\n",
      "y_train (134077 records): [5, 4]\n",
      "\n",
      "Reading dev input...\n",
      "...finished reading dev input, after 5 seconds\n",
      "X_dev (16760 records): [[4, 20, 6, 89017, 6992, 65, 8585, 185, 39, 148, 65, 67, 193, 68, 6, 89017, 349, 65, 342, 32326, 14, 6, 8174, 69, 50, 653, 80, 79, 6, 449, 65, 278, 218, 161, 303, 6992, 2213, 124, 15, 1140, 85, 7116, 150, 68, 42, 7383, 16, 69, 127, 5800, 12, 85, 6, 2877, 50112, 12644, 124, 15, 561, 3499, 85, 7116, 68, 6, 8005, 9, 858, 365, 53, 94, 954, 9018, 45479, 8263, 245, 50, 31, 114, 59273], [4, 79, 64, 733, 31, 40623, 32299, 65, 3430, 6992, 136, 8430, 100, 20, 291, 31, 546, 16, 69, 50, 1285, 77, 193, 321, 77, 3012]]\n",
      "y_dev (16760 records): [5, 3]\n",
      "\n",
      "Reading test input...\n",
      "...finished reading test input, after 5 seconds\n",
      "X_test (16759 records): [[357, 50, 535, 94, 126076, 84, 20, 69, 13921, 1787, 41, 9, 364, 365, 53, 102, 94, 1226, 68, 331120, 28786, 93, 127, 1369, 31, 6, 13921, 1542, 65, 181, 297, 4937, 31, 41, 3575, 68, 877], [69, 39, 158, 128, 65, 901, 12, 85, 193, 321, 68, 733, 31, 280, 2553, 245, 6, 3730, 65, 9687, 9, 1181, 365, 53, 280, 1849, 158, 94, 877, 185, 67, 68, 6, 126076, 5055, 4593, 3451, 65, 55945, 25, 23082, 14, 123, 6, 5055, 3451, 127, 80, 161, 4757, 307, 547, 6, 5055, 1369, 31, 123, 4, 591, 190, 1257, 245, 161, 5191, 16, 77, 1471, 69, 1031, 13921, 1542, 331121, 146, 158, 4095, 4, 167]]\n",
      "y_test (16759 records): [5, 4]\n",
      "vocab contains 364309 words.\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "\n",
    "print(\"\\nReading train input...\")\n",
    "start_time = time.time()\n",
    "X_train = [review for review in get_values(FILE_PATH, 'reviewText', [0,.8], vocab)]\n",
    "y_train = [int(overall) for overall in get_values(FILE_PATH, 'overall', [0,.8])]\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"...finished reading train input, after {round(elapsed)} seconds\")\n",
    "print(f\"X_train ({len(X_train)} records): {X_train[:2]}\")\n",
    "print(f\"y_train ({len(y_train)} records): {y_train[:2]}\")\n",
    "\n",
    "print(\"\\nReading dev input...\")\n",
    "start_time = time.time()\n",
    "X_dev = [review for review in get_values(FILE_PATH, 'reviewText', [.8,.9], vocab)]\n",
    "y_dev = [int(overall) for overall in get_values(FILE_PATH, 'overall', [.8,.9])]\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"...finished reading dev input, after {round(elapsed)} seconds\")\n",
    "print(f\"X_dev ({len(X_dev)} records): {X_dev[:2]}\")\n",
    "print(f\"y_dev ({len(y_dev)} records): {y_dev[:2]}\")\n",
    "\n",
    "print(\"\\nReading test input...\")\n",
    "start_time = time.time()\n",
    "X_test = [review for review in get_values(FILE_PATH, 'reviewText', [.9,1.0], vocab)]\n",
    "y_test = [int(overall) for overall in get_values(FILE_PATH, 'overall', [.9,1.0])]\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"...finished reading test input, after {round(elapsed)} seconds\")\n",
    "print(f\"X_test ({len(X_test)} records): {X_test[:2]}\")\n",
    "print(f\"y_test ({len(y_test)} records): {y_test[:2]}\")\n",
    "\n",
    "print(f\"vocab contains {len(vocab)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "#(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=NUM_WORDS, index_from=INDEX_FROM, skip_top=SKIP_TOP)\n",
    "\n",
    "# Sequences pre-processing\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different placeholders\n",
    "with tf.name_scope('Inputs'):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "with tf.name_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-06f3ed5d6bb5>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-06f3ed5d6bb5>:3: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_outputs:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Bi-)RNN layer(-s)\n",
    "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "tf.summary.histogram('RNN_outputs', rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-9e68b1111956>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Attention layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    tf.summary.histogram('alphas', alphas)\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(attention_output, keep_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "with tf.name_scope('Fully_connected_layer'):\n",
    "    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
    "    b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    tf.summary.histogram('W', W)\n",
    "\n",
    "with tf.name_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Batch generators\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/523 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "epoch: 0\tIndices: (134077 elemements) sample: [0 1 2 3 4 5 6 7 8 9]\n",
      "X_copy (134077 elements) sample: [[  4   5   6 ...   0   0   0]\n",
      " [ 20   6  21 ...   0   0   0]\n",
      " [ 37  38  39 ...   0   0   0]\n",
      " ...\n",
      " [  9  10 158 ...   0   0   0]\n",
      " [ 69 127 168 ...   0   0   0]\n",
      " [  4 195  69 ...   0   0   0]]\n",
      "y_copy (134077 elements) sample: [5 4 5 5 4 3 3 5 4 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 151/523 [05:11<12:25,  2.00s/it]"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Calculate alpha coefficients for the first test example\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "\n",
    "    x_batch_test, y_batch_test = X_test[:], y_test[:]\n",
    "    seq_len_test = np.array([list(x).index(0) + 1 for x in x_batch_test])\n",
    "    alphas_test = sess.run([alphas], feed_dict={batch_ph: x_batch_test, target_ph: y_batch_test,\n",
    "                                                seq_len_ph: seq_len_test, keep_prob_ph: 1.0})\n",
    "alphas_values = alphas_test[0][0]\n",
    "\n",
    "# Build correct mapping from word to index and inverse\n",
    "word_index = imdb.get_word_index()\n",
    "word_index = {word: index + INDEX_FROM for word, index in word_index.items()}\n",
    "word_index[\":PAD:\"] = 0\n",
    "word_index[\":START:\"] = 1\n",
    "word_index[\":UNK:\"] = 2\n",
    "index_word = {value: key for key, value in word_index.items()}\n",
    "# Represent the sample by words rather than indices\n",
    "words = list(map(index_word.get, x_batch_test[0]))\n",
    "\n",
    "# Save visualization as HTML\n",
    "with open(\"visualization.html\", \"w\") as html_file:\n",
    "    for word, alpha in sorted(zip(words, alphas_values / alphas_values.max()), \n",
    "                              key=lambda entry: -entry[1]):\n",
    "        if word == \":START:\":\n",
    "            continue\n",
    "        elif word == \":PAD:\":\n",
    "            break\n",
    "        html_file.write('<font style=\"background: rgba(255, 255, 0, %f)\" size=%f>%s (%f)<br></font>\\n' % (alpha, 2+alpha*2, word, alpha))\n",
    "\n",
    "print('\\nFollowing are words with comparative attention scores and visualization with color background.')\n",
    "\n",
    "with open(\"visualization.html\", \"r\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "\n",
    "HTML(filename=\"./visualization.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
