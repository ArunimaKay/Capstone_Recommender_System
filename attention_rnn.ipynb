{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train RNN (GRU) + Attention on Amason product review/rating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip install numpy\n",
    "# !sudo pip install tensorflow\n",
    "# !sudo pip install keras\n",
    "# !sudo pip install tqdmn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "import gzip\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention import attention\n",
    "from rnn_common.utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 40000\n",
    "INDEX_FROM = 3\n",
    "SKIP_TOP = 10 # This skips the 10 most frequent words in the IMDB reviews text\n",
    "SEQUENCE_LENGTH = 40\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 150\n",
    "ATTENTION_SIZE = 50\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "MODEL_PATH = './model'\n",
    "FILE_PATH = './data/reviews_Toys_and_Games_5.json.gz'\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_lines(filepath):\n",
    "    line_count = 0\n",
    "    with gzip.open(filepath) as input_file:\n",
    "        for line in input_file:\n",
    "            line_count += 1            \n",
    "    return line_count\n",
    "            \n",
    "\n",
    "def get_values(filepath, column, bounds, vocab=None, stopwords=None):\n",
    "    line_count = num_lines(filepath)\n",
    "    start_line = bounds[0] * line_count\n",
    "    end_line = bounds[1] * line_count\n",
    "\n",
    "    with gzip.open(filepath) as input_file:\n",
    "        curr_line = 0\n",
    "        for line in input_file:\n",
    "            curr_line += 1\n",
    "            if curr_line >= start_line and curr_line < end_line:\n",
    "                line_dict = eval(line)\n",
    "                line_val = line_dict[column]\n",
    "                \n",
    "                if vocab is not None:\n",
    "                    line_ids = []\n",
    "                    for word in line_val.lower().split(\" \"):\n",
    "                        if stopwords is not None:\n",
    "                            if word in stopwords:\n",
    "                                continue\n",
    "                        word_id = vocab.get(word)\n",
    "                        if word_id is None:\n",
    "                            word_id = len(vocab)+4\n",
    "                            vocab[word] = word_id\n",
    "                        line_ids.append(word_id)\n",
    "                    line_val = line_ids\n",
    "                \n",
    "                    yield len(line_val), line_val\n",
    "                else:\n",
    "                    yield line_val/5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering English stop Words (179 words)\n",
      "\n",
      "Reading train input...\n",
      "Percentiles (25%, 50%, 75%, 100%) seq_len (X_train) = [  18.   32.   62. 3168.]\n",
      "...finished reading train input, after 24 seconds\n",
      "X_train (134077 records): [[4, 5, 6, 7, 8, 9, 8, 10], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]]\n",
      "y_train (134077 records): [1, 0]\n",
      "\n",
      "Reading dev input...\n",
      "...finished reading dev input, after 5 seconds\n",
      "X_dev (16760 records): [[11, 88860, 6852, 8443, 104, 45, 140, 88860, 278, 274, 32171, 8033, 31, 373, 219, 162, 6852, 2088, 1031, 6976, 7243, 5663, 2750, 49956, 12501, 475, 6976, 7864, 757, 293, 34, 62, 8876, 45323, 8121, 31, 59117], [43, 637, 40467, 32144, 3300, 6852, 8288, 11, 462, 31, 140, 254, 2884]]\n",
      "y_dev (16760 records): [1, 0]\n",
      "\n",
      "Reading test input...\n",
      "...finished reading test input, after 5 seconds\n",
      "X_test (16759 records): [[31, 62, 125918, 11, 13773, 1669, 27, 292, 293, 34, 68, 62, 1114, 330954, 28631, 1255, 13773, 1426, 4802, 27, 3444, 775], [112, 88, 799, 140, 254, 637, 221, 2428, 3599, 9545, 1069, 293, 34, 221, 1729, 112, 62, 775, 45, 125918, 4920, 4459, 3321, 55789, 16, 22931, 85, 4920, 3321, 4623, 463, 4920, 1255, 85, 1145, 5056, 1355, 926, 13773, 1426, 330955, 112, 3962, 119]]\n",
      "y_test (16759 records): [1, 0]\n",
      "vocab contains 364143 words.\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "stopWords = set(stopwords.words('english'))\n",
    "print(f\"Filtering English stop Words ({len(stopWords)} words)\")\n",
    "\n",
    "print(\"\\nReading train input...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Note that the actual seq_len is the first of two values in the tokenized review input, the token list is the second\n",
    "percents_seq_len = np.percentile([review[0] for review in get_values(FILE_PATH, 'reviewText', [0,.8], \n",
    "                                                                     vocab, stopWords)],\n",
    "                                [25,50,75,100])\n",
    "print(f\"Percentiles (25%, 50%, 75%, 100%) seq_len (X_train) = {percents_seq_len}\")\n",
    "X_train = [review[1] for review in get_values(FILE_PATH, 'reviewText', [0,.8], vocab, stopWords)]\n",
    "y_train = [int(overall) for overall in get_values(FILE_PATH, 'overall', [0,.8])]\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"...finished reading train input, after {round(elapsed)} seconds\")\n",
    "print(f\"X_train ({len(X_train)} records): {X_train[:2]}\")\n",
    "print(f\"y_train ({len(y_train)} records): {y_train[:2]}\")\n",
    "\n",
    "print(\"\\nReading dev input...\")\n",
    "start_time = time.time()\n",
    "# Note that the actual seq_len is the first of two values in the tokenized review input, the token list is the second\n",
    "X_dev = [review[1] for review in get_values(FILE_PATH, 'reviewText', [.8,.9], vocab, stopWords)]\n",
    "y_dev = [int(overall) for overall in get_values(FILE_PATH, 'overall', [.8,.9])]\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"...finished reading dev input, after {round(elapsed)} seconds\")\n",
    "print(f\"X_dev ({len(X_dev)} records): {X_dev[:2]}\")\n",
    "print(f\"y_dev ({len(y_dev)} records): {y_dev[:2]}\")\n",
    "\n",
    "print(\"\\nReading test input...\")\n",
    "start_time = time.time()\n",
    "# Note that the actual seq_len is the first of two values in the tokenized review input, the token list is the second\n",
    "X_test = [review[1] for review in get_values(FILE_PATH, 'reviewText', [.9,1.0], vocab, stopWords)]\n",
    "y_test = [int(overall) for overall in get_values(FILE_PATH, 'overall', [.9,1.0])]\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"...finished reading test input, after {round(elapsed)} seconds\")\n",
    "print(f\"X_test ({len(X_test)} records): {X_test[:2]}\")\n",
    "print(f\"y_test ({len(y_test)} records): {y_test[:2]}\")\n",
    "\n",
    "print(f\"vocab contains {len(vocab)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "\n",
    "# Sequences pre-processing\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different placeholders\n",
    "with tf.name_scope('Inputs'):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "with tf.name_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([vocabulary_size, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-06f3ed5d6bb5>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-06f3ed5d6bb5>:3: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_outputs:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Bi-)RNN layer(-s)\n",
    "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "tf.summary.histogram('RNN_outputs', rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-9e68b1111956>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Attention layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    tf.summary.histogram('alphas', alphas)\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(attention_output, keep_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "with tf.name_scope('Fully_connected_layer'):\n",
    "    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
    "    b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    tf.summary.histogram('W', W)\n",
    "\n",
    "with tf.name_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    #loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=target_ph, predictions=y_hat))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Batch generators\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n",
    "\n",
    "train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/523 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "epoch: 0\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 13/523 [00:18<11:50,  1.39s/it]"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "vocab_ind = {}\n",
    "for word,ind in vocab.items():\n",
    "    vocab_ind[ind]=word\n",
    "              \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Calculate alpha coefficients for the first test example\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "\n",
    "    x_batch_test, y_batch_test = X_test[:], y_test[:]\n",
    "    seq_len_test = np.array([list(x).index(0) + 1 for x in x_batch_test])\n",
    "    alphas_test = sess.run([alphas], feed_dict={batch_ph: x_batch_test, target_ph: y_batch_test,\n",
    "                                                seq_len_ph: seq_len_test, keep_prob_ph: 1.0})\n",
    "alphas_values = alphas_test[0][0]\n",
    "\n",
    "# Build correct mapping from word to index and inverse\n",
    "word_index = vocab\n",
    "word_index = {word: index + INDEX_FROM for word, index in word_index.items()}\n",
    "word_index[\":PAD:\"] = 0\n",
    "word_index[\":START:\"] = 1\n",
    "word_index[\":UNK:\"] = 2\n",
    "index_word = {value: key for key, value in word_index.items()}\n",
    "# Represent the sample by words rather than indices\n",
    "words = list(map(index_word.get, x_batch_test[0]))\n",
    "\n",
    "# Save visualization as HTML\n",
    "with open(\"visualization.html\", \"w\") as html_file:\n",
    "    for word, alpha in sorted(zip(words, alphas_values / alphas_values.max()), \n",
    "                              key=lambda entry: -entry[1]):\n",
    "        if word == \":START:\":\n",
    "            continue\n",
    "        elif word == \":PAD:\":\n",
    "            break\n",
    "        html_file.write('<font style=\"background: rgba(255, 255, 0, %f)\" size=%f>%s (%f)<br></font>\\n' % (alpha, 2+alpha*2, word, alpha))\n",
    "\n",
    "print('\\nFollowing are words with comparative attention scores and visualization with color background.')\n",
    "\n",
    "with open(\"visualization.html\", \"r\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "\n",
    "HTML(filename=\"./visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
