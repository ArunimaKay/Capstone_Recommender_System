{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Train RNN (GRU) + Attention on Amason product review/rating data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo pip install numpy\n",
    "# !sudo pip install tensorflow\n",
    "# !sudo pip install keras\n",
    "# !sudo pip install tqdmn\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.contrib.rnn import GRUCell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn as bi_rnn\n",
    "import gzip\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from attention import attention\n",
    "from rnn_common.utils import get_vocabulary_size, fit_in_vocabulary, zero_pad, batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = -1\n",
    "INDEX_FROM = 3\n",
    "# SKIP_TOP = 0 # This will instead be handled by filtering stop words prior to training/testing\n",
    "SEQUENCE_LENGTH = 50 # The mean sequence length in training data is 32 (after filtering stop words)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_SIZE = 200\n",
    "ATTENTION_SIZE = SEQUENCE_LENGTH * 2\n",
    "KEEP_PROB = 0.8\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 3  # Model easily overfits without pre-trained words embeddings, that's why train for a few epochs\n",
    "DELTA = 0.5\n",
    "MODEL_PATH = './model'\n",
    "FILE_PATH = './data/reviews_Toys_and_Games_5.json.gz'\n",
    "UNKNOWN_WORDS_FILE_PATH = './data/unknown_words.txt'\n",
    "PAD_ID = 0\n",
    "START_ID = 1\n",
    "UNK_ID = 2\n",
    "\n",
    "train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_lines(filepath):\n",
    "    line_count = 0\n",
    "    with gzip.open(filepath) as input_file:\n",
    "        for line in input_file:\n",
    "            line_count += 1            \n",
    "    return line_count\n",
    "            \n",
    "\n",
    "def get_splits(filepath, split, split_add_words=None):\n",
    "    if sum(split) != 1.0:\n",
    "        raise ValueError(f\"Error, split items {split} do not add up to 1.\")\n",
    "        \n",
    "    # Default to adding words only from the train dataset\n",
    "    if split_add_words is None:\n",
    "        split_add_words = [False*len(split)]\n",
    "        split_add_words[0] = True\n",
    "        \n",
    "    vocab = {}\n",
    "    debug = True\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    print(f\"Filtering ({len(stopWords)}) English stop words)\")\n",
    "    \n",
    "    X_datasets = []\n",
    "    y_datasets = []\n",
    "    \n",
    "    # populate empty value lists for each split\n",
    "    for _ in range(len(split)):\n",
    "        X_datasets.append([])\n",
    "        y_datasets.append([])\n",
    "    \n",
    "    line_count = num_lines(filepath)\n",
    "    \n",
    "    split_arr = []\n",
    "    for ind in range(len(split)):\n",
    "        # append list of [0]s, [1]s, etc. each of the size indicated by split\n",
    "        item_count = int(split[ind]*line_count) + 1\n",
    "        print(f\"Appending {item_count} [{ind}]s for split {split[ind]}...\")\n",
    "        split_arr.extend([int(ind)]*item_count)\n",
    "        \n",
    "    split_arr = np.array(split_arr)\n",
    "    \n",
    "    # Shuffle the split inds just added\n",
    "    np.random.shuffle(split_arr)\n",
    "    \n",
    "    print(f\"Shuffled split_arr: {split_arr}\")\n",
    "    \n",
    "    ## unknown words file exists, delete it\n",
    "    if os.path.isfile(UNKNOWN_WORDS_FILE_PATH):\n",
    "        os.remove(UNKNOWN_WORDS_FILE_PATH)\n",
    "\n",
    "    with gzip.open(filepath) as input_file, open(UNKNOWN_WORDS_FILE_PATH, 'a') as unknown_file:\n",
    "        curr_line = 0\n",
    "        for line in input_file:\n",
    "            # Look back into the split list shuffled above to determine the appropriate split for this line\n",
    "            split_ind = split_arr[curr_line]\n",
    "            \n",
    "            if curr_line < 10:\n",
    "                print(f\"\\nLine {curr_line}:\", line)\n",
    "            else:\n",
    "                # Only enable debug output for at most the first 10 lines\n",
    "                debug = False\n",
    "                \n",
    "            # Add the review text to the appropriate split, only adding to the vocab as specified in split_add_words.\n",
    "            # Default behavior is to add words to vocab only for the first split (train)\n",
    "            word_list = get_values(line, 'reviewText', vocab=vocab, stopwords=stopWords,\n",
    "                                   add_words=(split_add_words[split_ind]), debug=debug, unknown_file=unknown_file)\n",
    "            if len(word_list) > 0:\n",
    "                X_datasets[split_ind].append(word_list)\n",
    "                y_datasets[split_ind].append(get_values(line, 'overall', debug=debug))\n",
    "                if debug:\n",
    "                    print(f\"split_ind: {split_ind}, curr_line: {curr_line}\")\n",
    "                    split_rec = len(X_datasets[split_ind])-1\n",
    "                    print(f\"X_datasets[{split_ind}][{split_rec}]: {X_datasets[split_ind][split_rec]}\")\n",
    "                    print(f\"y_datasets[{split_ind}][{split_rec}]: {y_datasets[split_ind][split_rec]}\")\n",
    "            \n",
    "            curr_line += 1\n",
    "                                        \n",
    "    print(\"Processed input file to produce the following randomized splits:\")\n",
    "    for ind in range(len(X_datasets)):\n",
    "        print(f\"[{ind}] : {len(X_datasets[ind])} samples\")\n",
    "        \n",
    "    return X_datasets, y_datasets, vocab\n",
    "\n",
    "    \n",
    "    \n",
    "def get_values(line, column, vocab=None, stopwords=None, add_words=False, debug=False, unknown_file=None):\n",
    "\n",
    "    line_dict = eval(line)\n",
    "    line_val = line_dict[column]\n",
    "    \n",
    "    \n",
    "    if vocab is not None:\n",
    "        line_ids = []\n",
    "        for word in line_val.lower().split(\" \"):\n",
    "            if (stopwords is not None) and (word in stopwords):\n",
    "                # skip stop words\n",
    "                continue\n",
    "                \n",
    "            word_id = vocab.get(word)\n",
    "            if word_id is None:\n",
    "                if add_words and ((NUM_WORDS<0) or (len(vocab)<(NUM_WORDS-1))):\n",
    "                    # Create a new word_id\n",
    "                    word_id = len(vocab)\n",
    "                    vocab[word] = word_id\n",
    "                    if debug:\n",
    "                        print(f\"Added word {word} with id {word_id}\")\n",
    "                else:\n",
    "                    # In the test dataset, unknown words will be skipped, but written to unknown file for review\n",
    "                    if unknown_file is not None:\n",
    "                        unknown_file.write(word)\n",
    "                    continue\n",
    "\n",
    "            line_ids.append(word_id)\n",
    "            if debug:\n",
    "                print(f\"IDS: {line_ids}\")\n",
    "                \n",
    "        # Start a word_id list with the actual seq_len\n",
    "        seq_len = [len(line_ids)]\n",
    "        line_val = line_ids\n",
    "        if debug:\n",
    "            print(f\"seq_len: {seq_len}\")\n",
    "            print(f\"line_val: {line_val}\")\n",
    "        \n",
    "        # For text values, the list of word ids will be returned\n",
    "        return line_ids\n",
    "    else:\n",
    "        # For the y values, an int value (converted from float) will be returned\n",
    "        return int(line_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading inputs...\n",
      "Filtering (179) English stop words)\n",
      "Appending 134078 [0]s for split 0.8...\n",
      "Appending 16760 [1]s for split 0.1...\n",
      "Appending 16760 [2]s for split 0.1...\n",
      "Shuffled split_arr: [0 0 0 ... 2 0 2]\n",
      "\n",
      "Line 0: b'{\"reviewerID\": \"A1VXOAVRGKGEAK\", \"asin\": \"0439893577\", \"reviewerName\": \"Angie\", \"helpful\": [0, 0], \"reviewText\": \"I like the item pricing. My granddaughter wanted to mark on it but I wanted it just for the letters.\", \"overall\": 5.0, \"summary\": \"Magnetic board\", \"unixReviewTime\": 1390953600, \"reviewTime\": \"01 29, 2014\"}\\n'\n",
      "Added word like with id 0\n",
      "IDS: [0]\n",
      "Added word item with id 1\n",
      "IDS: [0, 1]\n",
      "Added word pricing. with id 2\n",
      "IDS: [0, 1, 2]\n",
      "Added word granddaughter with id 3\n",
      "IDS: [0, 1, 2, 3]\n",
      "Added word wanted with id 4\n",
      "IDS: [0, 1, 2, 3, 4]\n",
      "Added word mark with id 5\n",
      "IDS: [0, 1, 2, 3, 4, 5]\n",
      "IDS: [0, 1, 2, 3, 4, 5, 4]\n",
      "Added word letters. with id 6\n",
      "IDS: [0, 1, 2, 3, 4, 5, 4, 6]\n",
      "seq_len: [8]\n",
      "line_val: [0, 1, 2, 3, 4, 5, 4, 6]\n",
      "split_ind: 0, curr_line: 0\n",
      "X_datasets[0][0]: [0, 1, 2, 3, 4, 5, 4, 6]\n",
      "y_datasets[0][0]: 5\n",
      "\n",
      "Line 1: b'{\"reviewerID\": \"A8R62G708TSCM\", \"asin\": \"0439893577\", \"reviewerName\": \"Candace\", \"helpful\": [1, 1], \"reviewText\": \"Love the magnet easel... great for moving to different areas... Wish it had some sort of non skid pad on bottom though...\", \"overall\": 4.0, \"summary\": \"it works pretty good for moving to different areas\", \"unixReviewTime\": 1395964800, \"reviewTime\": \"03 28, 2014\"}\\n'\n",
      "Added word love with id 7\n",
      "IDS: [7]\n",
      "Added word magnet with id 8\n",
      "IDS: [7, 8]\n",
      "Added word easel... with id 9\n",
      "IDS: [7, 8, 9]\n",
      "Added word great with id 10\n",
      "IDS: [7, 8, 9, 10]\n",
      "Added word moving with id 11\n",
      "IDS: [7, 8, 9, 10, 11]\n",
      "Added word different with id 12\n",
      "IDS: [7, 8, 9, 10, 11, 12]\n",
      "Added word areas... with id 13\n",
      "IDS: [7, 8, 9, 10, 11, 12, 13]\n",
      "Added word wish with id 14\n",
      "IDS: [7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Added word sort with id 15\n",
      "IDS: [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Added word non with id 16\n",
      "IDS: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "Added word skid with id 17\n",
      "IDS: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Added word pad with id 18\n",
      "IDS: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
      "Added word bottom with id 19\n",
      "IDS: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "Added word though... with id 20\n",
      "IDS: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "seq_len: [14]\n",
      "line_val: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "split_ind: 0, curr_line: 1\n",
      "X_datasets[0][1]: [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "y_datasets[0][1]: 4\n",
      "\n",
      "Line 2: b'{\"reviewerID\": \"A21KH420DK0ICA\", \"asin\": \"0439893577\", \"reviewerName\": \"capemaychristy\", \"helpful\": [1, 1], \"reviewText\": \"Both sides are magnetic.  A real plus when you\\'re entertaining more than one child.  The four-year old can find the letters for the words, while the two-year old can find the pictures the words spell.  (I bought letters and magnetic pictures to go with this board).  Both grandkids liked it a lot, which means I like it a lot as well.  Have not even introduced markers, as this will be used strictly as a magnetic board.\", \"overall\": 5.0, \"summary\": \"love this!\", \"unixReviewTime\": 1359331200, \"reviewTime\": \"01 28, 2013\"}\\n'\n",
      "Added word sides with id 21\n",
      "IDS: [21]\n",
      "Added word magnetic. with id 22\n",
      "IDS: [21, 22]\n",
      "Added word  with id 23\n",
      "IDS: [21, 22, 23]\n",
      "Added word real with id 24\n",
      "IDS: [21, 22, 23, 24]\n",
      "Added word plus with id 25\n",
      "IDS: [21, 22, 23, 24, 25]\n",
      "Added word entertaining with id 26\n",
      "IDS: [21, 22, 23, 24, 25, 26]\n",
      "Added word one with id 27\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27]\n",
      "Added word child. with id 28\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23]\n",
      "Added word four-year with id 29\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29]\n",
      "Added word old with id 30\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30]\n",
      "Added word find with id 31\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31]\n",
      "Added word letters with id 32\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32]\n",
      "Added word words, with id 33\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33]\n",
      "Added word two-year with id 34\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31]\n",
      "Added word pictures with id 35\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35]\n",
      "Added word words with id 36\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36]\n",
      "Added word spell. with id 37\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23]\n",
      "Added word (i with id 38\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38]\n",
      "Added word bought with id 39\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32]\n",
      "Added word magnetic with id 40\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35]\n",
      "Added word go with id 41\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41]\n",
      "Added word board). with id 42\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23]\n",
      "Added word grandkids with id 43\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43]\n",
      "Added word liked with id 44\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44]\n",
      "Added word lot, with id 45\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45]\n",
      "Added word means with id 46\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0]\n",
      "Added word lot with id 47\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47]\n",
      "Added word well. with id 48\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23]\n",
      "Added word even with id 49\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49]\n",
      "Added word introduced with id 50\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50]\n",
      "Added word markers, with id 51\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51]\n",
      "Added word used with id 52\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51, 52]\n",
      "Added word strictly with id 53\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51, 52, 53]\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51, 52, 53, 40]\n",
      "Added word board. with id 54\n",
      "IDS: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51, 52, 53, 40, 54]\n",
      "seq_len: [44]\n",
      "line_val: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51, 52, 53, 40, 54]\n",
      "split_ind: 0, curr_line: 2\n",
      "X_datasets[0][2]: [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51, 52, 53, 40, 54]\n",
      "y_datasets[0][2]: 5\n",
      "\n",
      "Line 3: b'{\"reviewerID\": \"AR29QK6HPFYZ4\", \"asin\": \"0439893577\", \"reviewerName\": \"dcrm\", \"helpful\": [0, 0], \"reviewText\": \"Bought one a few years ago for my daughter and she loves it, still using it today. For the holidays we bought one for our niece and she loved it too.\", \"overall\": 5.0, \"summary\": \"Daughters love it\", \"unixReviewTime\": 1391817600, \"reviewTime\": \"02 8, 2014\"}\\n'\n",
      "IDS: [39]\n",
      "IDS: [39, 27]\n",
      "Added word years with id 55\n",
      "IDS: [39, 27, 55]\n",
      "Added word ago with id 56\n",
      "IDS: [39, 27, 55, 56]\n",
      "Added word daughter with id 57\n",
      "IDS: [39, 27, 55, 56, 57]\n",
      "Added word loves with id 58\n",
      "IDS: [39, 27, 55, 56, 57, 58]\n",
      "Added word it, with id 59\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59]\n",
      "Added word still with id 60\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60]\n",
      "Added word using with id 61\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60, 61]\n",
      "Added word today. with id 62\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62]\n",
      "Added word holidays with id 63\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63]\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39]\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39, 27]\n",
      "Added word niece with id 64\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39, 27, 64]\n",
      "Added word loved with id 65\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39, 27, 64, 65]\n",
      "Added word too. with id 66\n",
      "IDS: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39, 27, 64, 65, 66]\n",
      "seq_len: [16]\n",
      "line_val: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39, 27, 64, 65, 66]\n",
      "split_ind: 0, curr_line: 3\n",
      "X_datasets[0][3]: [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39, 27, 64, 65, 66]\n",
      "y_datasets[0][3]: 5\n",
      "\n",
      "Line 4: b'{\"reviewerID\": \"ACCH8EOML6FN5\", \"asin\": \"0439893577\", \"reviewerName\": \"DoyZ\", \"helpful\": [1, 1], \"reviewText\": \"I have a stainless steel refrigerator therefore there are not much space for my son to play with his magnet. Brought this for him to put his magnet on. He enjoys sticking his magnet on it. Great to have so he can play with his alphabet magnets.\", \"overall\": 4.0, \"summary\": \"Great to have so he can play with his alphabet ...\", \"unixReviewTime\": 1399248000, \"reviewTime\": \"05 5, 2014\"}\\n'\n",
      "Added word stainless with id 67\n",
      "IDS: [67]\n",
      "Added word steel with id 68\n",
      "IDS: [67, 68]\n",
      "Added word refrigerator with id 69\n",
      "IDS: [67, 68, 69]\n",
      "Added word therefore with id 70\n",
      "IDS: [67, 68, 69, 70]\n",
      "Added word much with id 71\n",
      "IDS: [67, 68, 69, 70, 71]\n",
      "Added word space with id 72\n",
      "IDS: [67, 68, 69, 70, 71, 72]\n",
      "Added word son with id 73\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73]\n",
      "Added word play with id 74\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74]\n",
      "Added word magnet. with id 75\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75]\n",
      "Added word brought with id 76\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76]\n",
      "Added word put with id 77\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77]\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8]\n",
      "Added word on. with id 78\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78]\n",
      "Added word enjoys with id 79\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79]\n",
      "Added word sticking with id 80\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80]\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8]\n",
      "Added word it. with id 81\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81]\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81, 10]\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81, 10, 74]\n",
      "Added word alphabet with id 82\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81, 10, 74, 82]\n",
      "Added word magnets. with id 83\n",
      "IDS: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81, 10, 74, 82, 83]\n",
      "seq_len: [21]\n",
      "line_val: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81, 10, 74, 82, 83]\n",
      "split_ind: 0, curr_line: 4\n",
      "X_datasets[0][4]: [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81, 10, 74, 82, 83]\n",
      "y_datasets[0][4]: 4\n",
      "\n",
      "Line 5: b'{\"reviewerID\": \"A1EDPGHC9EYBUN\", \"asin\": \"0439893577\", \"reviewerName\": \"kp\", \"helpful\": [0, 0], \"reviewText\": \"this is a nice magnetic board for the kids to carry around and play with the letter bought additional magnetic pieces from mellissa and doug to increase thier play time with them hope it works\", \"overall\": 3.0, \"summary\": \"grandma\", \"unixReviewTime\": 1368662400, \"reviewTime\": \"05 16, 2013\"}\\n'\n",
      "Added word nice with id 84\n",
      "IDS: [84]\n",
      "IDS: [84, 40]\n",
      "Added word board with id 85\n",
      "IDS: [84, 40, 85]\n",
      "Added word kids with id 86\n",
      "IDS: [84, 40, 85, 86]\n",
      "Added word carry with id 87\n",
      "IDS: [84, 40, 85, 86, 87]\n",
      "Added word around with id 88\n",
      "IDS: [84, 40, 85, 86, 87, 88]\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74]\n",
      "Added word letter with id 89\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89]\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39]\n",
      "Added word additional with id 90\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90]\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40]\n",
      "Added word pieces with id 91\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91]\n",
      "Added word mellissa with id 92\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92]\n",
      "Added word doug with id 93\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93]\n",
      "Added word increase with id 94\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93, 94]\n",
      "Added word thier with id 95\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93, 94, 95]\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93, 94, 95, 74]\n",
      "Added word time with id 96\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93, 94, 95, 74, 96]\n",
      "Added word hope with id 97\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93, 94, 95, 74, 96, 97]\n",
      "Added word works with id 98\n",
      "IDS: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93, 94, 95, 74, 96, 97, 98]\n",
      "seq_len: [20]\n",
      "line_val: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93, 94, 95, 74, 96, 97, 98]\n",
      "split_ind: 0, curr_line: 5\n",
      "X_datasets[0][5]: [84, 40, 85, 86, 87, 88, 74, 89, 39, 90, 40, 91, 92, 93, 94, 95, 74, 96, 97, 98]\n",
      "y_datasets[0][5]: 3\n",
      "\n",
      "Line 6: b'{\"reviewerID\": \"A181ZNB42LISRZ\", \"asin\": \"0439893577\", \"reviewerName\": \"laura\", \"helpful\": [0, 0], \"reviewText\": \"This is just as I expected. It\\'s not too sturdy but it stands up well. I\\'m not sure how long it will last, but my grandsons love plalying with it.\", \"overall\": 3.0, \"summary\": \"Serves the purpose\", \"unixReviewTime\": 1356480000, \"reviewTime\": \"12 26, 2012\"}\\n'\n",
      "IDS: [48]\n",
      "IDS: [48, 7]\n",
      "IDS: [48, 7, 81]\n",
      "seq_len: [3]\n",
      "line_val: [48, 7, 81]\n",
      "split_ind: 2, curr_line: 6\n",
      "X_datasets[2][0]: [48, 7, 81]\n",
      "y_datasets[2][0]: 3\n",
      "\n",
      "Line 7: b'{\"reviewerID\": \"A1RBEJ5UQ1RWAW\", \"asin\": \"0439893577\", \"reviewerName\": \"Linda\", \"helpful\": [1, 1], \"reviewText\": \"My granddaughter really really likes this. I love that you can just fold it up and put it away. Would definately recommend.\", \"overall\": 5.0, \"summary\": \"Great\", \"unixReviewTime\": 1397520000, \"reviewTime\": \"04 15, 2014\"}\\n'\n",
      "IDS: [3]\n",
      "Added word really with id 99\n",
      "IDS: [3, 99]\n",
      "IDS: [3, 99, 99]\n",
      "Added word likes with id 100\n",
      "IDS: [3, 99, 99, 100]\n",
      "Added word this. with id 101\n",
      "IDS: [3, 99, 99, 100, 101]\n",
      "IDS: [3, 99, 99, 100, 101, 7]\n",
      "Added word fold with id 102\n",
      "IDS: [3, 99, 99, 100, 101, 7, 102]\n",
      "IDS: [3, 99, 99, 100, 101, 7, 102, 77]\n",
      "Added word away. with id 103\n",
      "IDS: [3, 99, 99, 100, 101, 7, 102, 77, 103]\n",
      "Added word would with id 104\n",
      "IDS: [3, 99, 99, 100, 101, 7, 102, 77, 103, 104]\n",
      "Added word definately with id 105\n",
      "IDS: [3, 99, 99, 100, 101, 7, 102, 77, 103, 104, 105]\n",
      "Added word recommend. with id 106\n",
      "IDS: [3, 99, 99, 100, 101, 7, 102, 77, 103, 104, 105, 106]\n",
      "seq_len: [12]\n",
      "line_val: [3, 99, 99, 100, 101, 7, 102, 77, 103, 104, 105, 106]\n",
      "split_ind: 0, curr_line: 7\n",
      "X_datasets[0][6]: [3, 99, 99, 100, 101, 7, 102, 77, 103, 104, 105, 106]\n",
      "y_datasets[0][6]: 5\n",
      "\n",
      "Line 8: b'{\"reviewerID\": \"AFGGC7SIV964O\", \"asin\": \"0439893577\", \"reviewerName\": \"Maan\\'s Mom\", \"helpful\": [0, 0], \"reviewText\": \"This is good product to have with unlimited possibilities. I also ordered the magnetic letters & numbers by Melissa & Doug.  The dry erase has to be erased quickly or they tend to leave marks. Also, the magnet is not very strong. It\\'s a nice size & works very well for our needs.\", \"overall\": 4.0, \"summary\": \"A must have for any homeschooling classroom !\", \"unixReviewTime\": 1364428800, \"reviewTime\": \"03 28, 2013\"}\\n'\n",
      "Added word good with id 107\n",
      "IDS: [107]\n",
      "Added word product with id 108\n",
      "IDS: [107, 108]\n",
      "Added word unlimited with id 109\n",
      "IDS: [107, 108, 109]\n",
      "Added word possibilities. with id 110\n",
      "IDS: [107, 108, 109, 110]\n",
      "Added word also with id 111\n",
      "IDS: [107, 108, 109, 110, 111]\n",
      "Added word ordered with id 112\n",
      "IDS: [107, 108, 109, 110, 111, 112]\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40]\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32]\n",
      "Added word & with id 113\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113]\n",
      "Added word numbers with id 114\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114]\n",
      "Added word melissa with id 115\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115]\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113]\n",
      "Added word doug. with id 116\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116]\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23]\n",
      "Added word dry with id 117\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117]\n",
      "Added word erase with id 118\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118]\n",
      "Added word erased with id 119\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119]\n",
      "Added word quickly with id 120\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120]\n",
      "Added word tend with id 121\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121]\n",
      "Added word leave with id 122\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122]\n",
      "Added word marks. with id 123\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123]\n",
      "Added word also, with id 124\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124]\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8]\n",
      "Added word strong. with id 125\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125]\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125, 84]\n",
      "Added word size with id 126\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125, 84, 126]\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125, 84, 126, 113]\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125, 84, 126, 113, 98]\n",
      "Added word well with id 127\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125, 84, 126, 113, 98, 127]\n",
      "Added word needs. with id 128\n",
      "IDS: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125, 84, 126, 113, 98, 127, 128]\n",
      "seq_len: [30]\n",
      "line_val: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125, 84, 126, 113, 98, 127, 128]\n",
      "split_ind: 0, curr_line: 8\n",
      "X_datasets[0][7]: [107, 108, 109, 110, 111, 112, 40, 32, 113, 114, 115, 113, 116, 23, 117, 118, 119, 120, 121, 122, 123, 124, 8, 125, 84, 126, 113, 98, 127, 128]\n",
      "y_datasets[0][7]: 4\n",
      "\n",
      "Line 9: b'{\"reviewerID\": \"A2XU46XXNV19C8\", \"asin\": \"0439893577\", \"reviewerName\": \"Margaret\", \"helpful\": [59, 59], \"reviewText\": \"I keep this board on top of the hallway table so that I can quickly write notes (which won\\'t get lost until I erase them) and it folds down neatly so it\\'s easy to hide when company calls.  I also like the size -- not too big and not too small.  Quality magnets hold pretty good but I use it mainly for notes. NOTE:  Children\\'s alphabet letters don\\'t hold very well - they tend to slide.\", \"overall\": 3.0, \"summary\": \"Nifty little thing\", \"unixReviewTime\": 1258588800, \"reviewTime\": \"11 19, 2009\"}\\n'\n",
      "Added word keep with id 129\n",
      "IDS: [129]\n",
      "IDS: [129, 85]\n",
      "Added word top with id 130\n",
      "IDS: [129, 85, 130]\n",
      "Added word hallway with id 131\n",
      "IDS: [129, 85, 130, 131]\n",
      "Added word table with id 132\n",
      "IDS: [129, 85, 130, 131, 132]\n",
      "IDS: [129, 85, 130, 131, 132, 120]\n",
      "Added word write with id 133\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133]\n",
      "Added word notes with id 134\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134]\n",
      "Added word (which with id 135\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135]\n",
      "Added word get with id 136\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136]\n",
      "Added word lost with id 137\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118]\n",
      "Added word them) with id 138\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138]\n",
      "Added word folds with id 139\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139]\n",
      "Added word neatly with id 140\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140]\n",
      "Added word easy with id 141\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141]\n",
      "Added word hide with id 142\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142]\n",
      "Added word company with id 143\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143]\n",
      "Added word calls. with id 144\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126]\n",
      "Added word -- with id 145\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145]\n",
      "Added word big with id 146\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146]\n",
      "Added word small. with id 147\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23]\n",
      "Added word quality with id 148\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148]\n",
      "Added word magnets with id 149\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149]\n",
      "Added word hold with id 150\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150]\n",
      "Added word pretty with id 151\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107]\n",
      "Added word use with id 152\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152]\n",
      "Added word mainly with id 153\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153]\n",
      "Added word notes. with id 154\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154]\n",
      "Added word note: with id 155\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23]\n",
      "Added word children's with id 156\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82, 32]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82, 32, 150]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82, 32, 150, 127]\n",
      "Added word - with id 157\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82, 32, 150, 127, 157]\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82, 32, 150, 127, 157, 121]\n",
      "Added word slide. with id 158\n",
      "IDS: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82, 32, 150, 127, 157, 121, 158]\n",
      "seq_len: [45]\n",
      "line_val: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82, 32, 150, 127, 157, 121, 158]\n",
      "split_ind: 0, curr_line: 9\n",
      "X_datasets[0][8]: [129, 85, 130, 131, 132, 120, 133, 134, 135, 136, 137, 118, 138, 139, 140, 141, 142, 143, 144, 23, 111, 0, 126, 145, 146, 147, 23, 148, 149, 150, 151, 107, 152, 153, 154, 155, 23, 156, 82, 32, 150, 127, 157, 121, 158]\n",
      "y_datasets[0][8]: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed input file to produce the following randomized splits:\n",
      "[0] : 134078 samples\n",
      "[1] : 16760 samples\n",
      "[2] : 16759 samples\n",
      "...finished reading input file, after 17 seconds\n",
      "Results:\n",
      "    X_datasets : 3 elements\n",
      "\n",
      "X_train (134078 records): [[0, 1, 2, 3, 4, 5, 4, 6], [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], [21, 22, 23, 24, 25, 26, 27, 28, 23, 29, 30, 31, 32, 33, 34, 30, 31, 35, 36, 37, 23, 38, 39, 32, 40, 35, 41, 42, 23, 43, 44, 45, 46, 0, 47, 48, 23, 49, 50, 51, 52, 53, 40, 54], [39, 27, 55, 56, 57, 58, 59, 60, 61, 62, 63, 39, 27, 64, 65, 66], [67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 8, 78, 79, 80, 8, 81, 10, 74, 82, 83]]\n",
      "y_train (134078 records): [5, 4, 5, 5, 4]\n",
      "Percentiles (25%, 50%, 75%, 100%) review_length (X_train) = [  18.   33.   65. 3168.]\n",
      "\n",
      "X_test (16760 records): [[313, 541, 166, 542, 273, 543, 544, 545, 546, 23, 341, 547, 52, 347, 548, 549, 550, 23, 195, 196, 551, 431, 176, 286, 552], [107, 656, 288, 10, 514, 23, 39, 657, 281, 30, 104, 658, 659, 660, 661, 662, 663, 23, 646, 104, 232, 193, 664, 99, 665], [900, 872, 838, 112, 901, 902, 903, 904, 44, 71, 243, 206, 112, 899, 905, 906, 907, 908, 518, 244, 462, 909, 910, 73, 911, 220, 912, 10, 913, 914, 915, 916, 265, 392, 917, 127, 918, 919, 23, 920, 814, 265, 386, 646, 921, 922, 291, 300, 41, 882, 162, 141, 923, 924, 925, 61, 926, 927, 928, 47, 260, 60, 929, 930, 931, 889, 932, 127, 484, 933, 347, 857, 560, 462, 300, 632, 47, 152, 469, 281, 934, 23, 44, 935, 112, 936, 882, 937, 938, 789, 0, 10, 939, 940, 941, 104, 195, 196, 632, 942, 943, 41, 101, 52, 944, 676, 945, 946, 947, 877, 948, 688, 949, 221, 765, 52, 950, 195, 951, 952, 666, 734, 953, 332, 954, 597, 10, 955], [902, 207, 59, 1093, 714, 104, 344, 1094, 23, 720, 1095, 187, 245, 23, 1096, 208, 260, 1097, 0, 1098, 1099, 1100, 1101, 1102, 659, 881, 23, 1103, 1078, 633], [1074, 10, 773, 1106, 773, 1107, 58, 1108, 666, 1109, 480, 47, 335, 595, 1107, 7, 1110, 1111, 1112, 107, 1113, 836, 1114, 96, 1115, 1116]]\n",
      "y_test (16760 records): [5, 5, 5, 5, 5]\n",
      "\n",
      "X_unk (16759 records): [[48, 7, 81], [170, 74, 71, 49, 258, 780, 386], [71, 291, 780, 622, 299, 796, 10, 299, 141], [877, 232, 172, 23, 386, 49, 71, 14, 104, 23, 58, 265, 49, 180, 23, 849, 882, 690, 882, 27, 129, 146], [73, 58, 999, 113, 0, 10, 433, 1004, 101, 23, 433, 40, 113, 386, 287, 12, 999, 507, 890, 150, 70, 518, 571, 81]]\n",
      "y_unk (16759 records): [3, 3, 5, 5, 3]\n",
      "\n",
      "vocab contains 339412 words.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReading inputs...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Split inputs into X_train, X_test, and X_unk. The first two will add to vocabulary, the third will not.\n",
    "X_datasets, y_datasets, vocab = get_splits(FILE_PATH, [.8,.1,.1], [True, True, False])\n",
    "elapsed = time.time()-start_time\n",
    "print(f\"...finished reading input file, after {round(elapsed)} seconds\")\n",
    "\n",
    "print(f\"Results:\")\n",
    "print(f\"    X_datasets : {len(X_datasets)} elements\")\n",
    "\n",
    "X_train = X_datasets[0]\n",
    "y_train = y_datasets[0]\n",
    "print(f\"\\nX_train ({len(X_train)} records): {X_train[:5]}\")\n",
    "print(f\"y_train ({len(y_train)} records): {y_train[:5]}\")\n",
    "\n",
    "percents_review_len = np.percentile([len(review) for review in X_train], [25,50,75,100])\n",
    "print(f\"Percentiles (25%, 50%, 75%, 100%) review_length (X_train) = {percents_review_len}\")\n",
    "\n",
    "X_test = X_datasets[1]\n",
    "y_test = y_datasets[1]\n",
    "print(f\"\\nX_test ({len(X_test)} records): {X_test[:5]}\")\n",
    "print(f\"y_test ({len(y_test)} records): {y_test[:5]}\")\n",
    "\n",
    "X_unk = X_datasets[2]\n",
    "y_unk = y_datasets[2]\n",
    "print(f\"\\nX_unk ({len(X_unk)} records): {X_unk[:5]}\")\n",
    "print(f\"y_unk ({len(y_unk)} records): {y_unk[:5]}\")\n",
    "\n",
    "print(f\"\\nvocab contains {len(vocab)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4, 5, 4, 6], [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]]\n",
      "0 entries in X_train have length 0\n"
     ]
    }
   ],
   "source": [
    "# Load the data set\n",
    "\n",
    "# Sequences pre-processing\n",
    "print(X_train[:2][:5])\n",
    "print(f\"{sum(1 for x in X_train if ((len(x) == 0) or (x is None)))} entries in X_train have length 0\")\n",
    "vocabulary_size = get_vocabulary_size(X_train)\n",
    "X_test = fit_in_vocabulary(X_test, vocabulary_size)\n",
    "X_train = zero_pad(X_train, SEQUENCE_LENGTH)\n",
    "X_test = zero_pad(X_test, SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different placeholders\n",
    "with tf.name_scope('Inputs'):\n",
    "    batch_ph = tf.placeholder(tf.int32, [None, SEQUENCE_LENGTH], name='batch_ph')\n",
    "    target_ph = tf.placeholder(tf.float32, [None], name='target_ph')\n",
    "    seq_len_ph = tf.placeholder(tf.int32, [None], name='seq_len_ph')\n",
    "    keep_prob_ph = tf.placeholder(tf.float32, name='keep_prob_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# Embedding layer\n",
    "with tf.name_scope('Embedding_layer'):\n",
    "    embeddings_var = tf.Variable(tf.random_uniform([len(vocab)+1, EMBEDDING_DIM], -1.0, 1.0), trainable=True)\n",
    "    tf.summary.histogram('embeddings_var', embeddings_var)\n",
    "    batch_embedded = tf.nn.embedding_lookup(embeddings_var, batch_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-06f3ed5d6bb5>:2: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-9-06f3ed5d6bb5>:3: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/burgew/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'RNN_outputs:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (Bi-)RNN layer(-s)\n",
    "rnn_outputs, _ = bi_rnn(GRUCell(HIDDEN_SIZE), GRUCell(HIDDEN_SIZE),\n",
    "                        inputs=batch_embedded, sequence_length=seq_len_ph, dtype=tf.float32)\n",
    "tf.summary.histogram('RNN_outputs', rnn_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-9e68b1111956>:7: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "# Attention layer\n",
    "with tf.name_scope('Attention_layer'):\n",
    "    attention_output, alphas = attention(rnn_outputs, ATTENTION_SIZE, return_alphas=True)\n",
    "    tf.summary.histogram('alphas', alphas)\n",
    "\n",
    "# Dropout\n",
    "drop = tf.nn.dropout(attention_output, keep_prob_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected layer\n",
    "with tf.name_scope('Fully_connected_layer'):\n",
    "    W = tf.Variable(tf.truncated_normal([HIDDEN_SIZE * 2, 1], stddev=0.1))  # Hidden size is multiplied by 2 for Bi-RNN\n",
    "    b = tf.Variable(tf.constant(0., shape=[1]))\n",
    "    y_hat = tf.nn.xw_plus_b(drop, W, b)\n",
    "    y_hat = tf.squeeze(y_hat)\n",
    "    tf.summary.histogram('W', W)\n",
    "\n",
    "with tf.name_scope('Metrics'):\n",
    "    # Cross-entropy loss and optimizer initialization\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=y_hat, labels=target_ph))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)\n",
    "\n",
    "    # Accuracy metric\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(tf.sigmoid(y_hat)), target_ph), tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Batch generators\n",
    "train_batch_generator = batch_generator(X_train, y_train, BATCH_SIZE)\n",
    "test_batch_generator = batch_generator(X_test, y_test, BATCH_SIZE)\n",
    "unk_batch_generator = batch_generator(X_unk, y_unk, BATCH_SIZE)\n",
    "\n",
    "train_writer = tf.summary.FileWriter('./logdir/train', accuracy.graph)\n",
    "test_writer = tf.summary.FileWriter('./logdir/test', accuracy.graph)\n",
    "unk_writer = tf.summary.FileWriter('./logdir/unk', accuracy.graph)\n",
    "\n",
    "session_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/523 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning...\n",
      "\n",
      "*********************************************\n",
      "epoch: 0\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|| 480/523 [15:13<01:22,  1.91s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d9d1bc1414bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                                \u001b[0mtarget_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                                                \u001b[0mseq_len_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                                                                keep_prob_ph: KEEP_PROB})\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0maccuracy_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_tr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mDELTA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mDELTA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    with tf.Session(config=session_conf) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Start learning...\")\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            loss_train = 0\n",
    "            loss_test = 0\n",
    "            accuracy_train = 0\n",
    "            accuracy_test = 0\n",
    "            accuracy_unk = 0\n",
    "\n",
    "            print(\"\\n*********************************************\")\n",
    "            print(\"epoch: {}\\t\".format(epoch), end=\"\")\n",
    "\n",
    "            # Training\n",
    "            print(\"\\nTraining...\")\n",
    "            num_batches = X_train.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(train_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_tr, acc, _, summary = sess.run([loss, accuracy, optimizer, merged],\n",
    "                                                    feed_dict={batch_ph: x_batch,\n",
    "                                                               target_ph: y_batch,\n",
    "                                                               seq_len_ph: seq_len,\n",
    "                                                               keep_prob_ph: KEEP_PROB})\n",
    "                accuracy_train += acc\n",
    "                loss_train = loss_tr * DELTA + loss_train * (1 - DELTA)\n",
    "                train_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_train /= num_batches\n",
    "\n",
    "            # Testing\n",
    "            print(\"\\nTesting with known words...\")\n",
    "            num_batches = X_test.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(test_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_test += acc\n",
    "                loss_test += loss_test_batch\n",
    "                test_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_test /= num_batches\n",
    "            loss_test /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}\\n\".format(\n",
    "                loss_train, loss_test, accuracy_train, accuracy_test\n",
    "            ))\n",
    "            \n",
    "            # Testing with unknown words\n",
    "            print(\"\\nTesting with unknown words...\")\n",
    "            num_batches = X_unk.shape[0] // BATCH_SIZE\n",
    "            for b in tqdm(range(num_batches)):\n",
    "                x_batch, y_batch = next(unk_batch_generator)\n",
    "                seq_len = np.array([list(x).index(0) + 1 for x in x_batch])  # actual lengths of sequences\n",
    "                loss_test_batch, acc, summary = sess.run([loss, accuracy, merged],\n",
    "                                                         feed_dict={batch_ph: x_batch,\n",
    "                                                                    target_ph: y_batch,\n",
    "                                                                    seq_len_ph: seq_len,\n",
    "                                                                    keep_prob_ph: 1.0})\n",
    "                accuracy_unk += acc\n",
    "                loss_unk += loss_unk_batch\n",
    "                unk_writer.add_summary(summary, b + num_batches * epoch)\n",
    "            accuracy_unk /= num_batches\n",
    "            loss_unk /= num_batches\n",
    "\n",
    "            print(\"loss: {:.3f}, val_loss: {:.3f}, unk_loss: {:.3f}, acc: {:.3f}, val_acc: {:.3f}, unk_acc: {:3f}\\n\".format(\n",
    "                loss_train, loss_test, loss_unk, accuracy_train, accuracy_test, accuracy_unk\n",
    "            ))\n",
    "        train_writer.close()\n",
    "        test_writer.close()\n",
    "        unk_writer.close()\n",
    "        saver.save(sess, MODEL_PATH)\n",
    "        print(\"Run 'tensorboard --logdir=./logdir' to checkout tensorboard logs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "vocab_ind = {}\n",
    "for word,ind in vocab.items():\n",
    "    vocab_ind[ind]=word\n",
    "    \n",
    "print(f\"Just created id->word dict of size {len(vocab_ind)} items\")\n",
    "print(f\" .   max(ind) = {max(vocab_ind.keys())}\")\n",
    "print(f\"Source was word->id dict of size {len(vocab)} items\")\n",
    "print(f\" .   max(ind) = {max(vocab.values())}\")\n",
    "              \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Calculate alpha coefficients for the first test example\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "\n",
    "    x_batch_test, y_batch_test = X_test[:], y_test[:]\n",
    "    seq_len_test = np.array([list(x).index(0) + 1 for x in x_batch_test])\n",
    "    alphas_test = sess.run([alphas], feed_dict={batch_ph: x_batch_test, target_ph: y_batch_test,\n",
    "                                                seq_len_ph: seq_len_test, keep_prob_ph: 1.0})\n",
    "alphas_values = alphas_test[0][0]\n",
    "\n",
    "# Build correct mapping from word to index and inverse\n",
    "word_index = vocab\n",
    "word_index = {word: index + INDEX_FROM for word, index in word_index.items()}\n",
    "word_index[\":PAD:\"] = PAD_ID\n",
    "word_index[\":START:\"] = START_ID\n",
    "word_index[\":UNK:\"] = UNK_ID\n",
    "index_word = {value: key for key, value in word_index.items()}\n",
    "# Represent the sample by words rather than indices\n",
    "words = list(map(index_word.get, x_batch_test[0]))\n",
    "\n",
    "# Save visualization as HTML\n",
    "with open(\"visualization.html\", \"w\") as html_file:\n",
    "    for word, alpha in sorted(zip(words, alphas_values / alphas_values.max()), \n",
    "                              key=lambda entry: -entry[1]):\n",
    "        if word == \":START:\":\n",
    "            continue\n",
    "        elif word == \":PAD:\":\n",
    "            break\n",
    "        html_file.write('<font style=\"background: rgba(255, 255, 0, %f)\" size=%f>%s (%f)<br></font>\\n' % (alpha, 2+alpha*2, word, alpha))\n",
    "\n",
    "print('\\nFollowing are words with comparative attention scores and visualization with color background.')\n",
    "\n",
    "with open(\"visualization.html\", \"r\") as html_file:\n",
    "    html_content = html_file.read()\n",
    "\n",
    "HTML(filename=\"./visualization.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
